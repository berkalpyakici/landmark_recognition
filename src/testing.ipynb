{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/93 [00:16<00:13,  4.14it/s]\n",
      "Loss: 3.74608:  43%|████▎     | 40/93 [00:16<00:14,  3.75it/s]\n",
      "Loss: 4.25185:  43%|████▎     | 40/93 [00:16<00:14,  3.75it/s]\n",
      "Loss: 4.25185:  44%|████▍     | 41/93 [00:16<00:12,  4.19it/s]\n",
      "Loss: 6.63773:  44%|████▍     | 41/93 [00:16<00:12,  4.19it/s]\n",
      "Loss: 6.63773:  45%|████▌     | 42/93 [00:16<00:11,  4.29it/s]\n",
      "Loss: 3.40441:  45%|████▌     | 42/93 [00:17<00:11,  4.29it/s]\n",
      "Loss: 3.40441:  46%|████▌     | 43/93 [00:17<00:10,  4.70it/s]\n",
      "Loss: 4.06006:  46%|████▌     | 43/93 [00:17<00:10,  4.70it/s]\n",
      "Loss: 4.06006:  47%|████▋     | 44/93 [00:17<00:11,  4.15it/s]\n",
      "Loss: 2.89536:  47%|████▋     | 44/93 [00:17<00:11,  4.15it/s]\n",
      "Loss: 2.89536:  48%|████▊     | 45/93 [00:17<00:10,  4.52it/s]\n",
      "Loss: 9.96201:  48%|████▊     | 45/93 [00:17<00:10,  4.52it/s]\n",
      "Loss: 9.96201:  49%|████▉     | 46/93 [00:17<00:12,  3.73it/s]\n",
      "Loss: 7.12583:  49%|████▉     | 46/93 [00:18<00:12,  3.73it/s]\n",
      "Loss: 7.12583:  51%|█████     | 47/93 [00:18<00:11,  4.15it/s]\n",
      "Loss: 7.31117:  51%|█████     | 47/93 [00:18<00:11,  4.15it/s]\n",
      "Loss: 7.31117:  52%|█████▏    | 48/93 [00:18<00:10,  4.30it/s]\n",
      "Loss: 0.93135:  52%|█████▏    | 48/93 [00:18<00:10,  4.30it/s]\n",
      "Loss: 0.93135:  53%|█████▎    | 49/93 [00:18<00:09,  4.58it/s]\n",
      "Loss: 9.34140:  53%|█████▎    | 49/93 [00:18<00:09,  4.58it/s]\n",
      "Loss: 9.34140:  54%|█████▍    | 50/93 [00:18<00:12,  3.52it/s]\n",
      "Loss: 8.76056:  54%|█████▍    | 50/93 [00:19<00:12,  3.52it/s]\n",
      "Loss: 8.76056:  55%|█████▍    | 51/93 [00:19<00:10,  3.94it/s]\n",
      "Loss: 5.76709:  55%|█████▍    | 51/93 [00:19<00:10,  3.94it/s]\n",
      "Loss: 5.76709:  56%|█████▌    | 52/93 [00:19<00:09,  4.17it/s]\n",
      "Loss: 4.16003:  56%|█████▌    | 52/93 [00:19<00:09,  4.17it/s]\n",
      "Loss: 4.16003:  57%|█████▋    | 53/93 [00:19<00:08,  4.48it/s]\n",
      "Loss: 8.65481:  57%|█████▋    | 53/93 [00:20<00:08,  4.48it/s]\n",
      "Loss: 8.65481:  58%|█████▊    | 54/93 [00:20<00:11,  3.38it/s]\n",
      "Loss: 6.28781:  58%|█████▊    | 54/93 [00:20<00:11,  3.38it/s]\n",
      "Loss: 6.28781:  59%|█████▉    | 55/93 [00:20<00:09,  3.86it/s]\n",
      "Loss: 3.26640:  59%|█████▉    | 55/93 [00:20<00:09,  3.86it/s]\n",
      "Loss: 3.26640:  60%|██████    | 56/93 [00:20<00:08,  4.28it/s]\n",
      "Loss: 4.52158:  60%|██████    | 56/93 [00:20<00:08,  4.28it/s]\n",
      "Loss: 4.52158:  61%|██████▏   | 57/93 [00:20<00:07,  4.64it/s]\n",
      "Loss: 3.08472:  61%|██████▏   | 57/93 [00:21<00:07,  4.64it/s]\n",
      "Loss: 3.08472:  62%|██████▏   | 58/93 [00:21<00:11,  3.05it/s]\n",
      "Loss: 2.60827:  62%|██████▏   | 58/93 [00:21<00:11,  3.05it/s]\n",
      "Loss: 2.60827:  63%|██████▎   | 59/93 [00:21<00:09,  3.53it/s]\n",
      "Loss: 3.52691:  63%|██████▎   | 59/93 [00:21<00:09,  3.53it/s]\n",
      "Loss: 3.52691:  65%|██████▍   | 60/93 [00:21<00:08,  3.98it/s]\n",
      "Loss: 3.22524:  65%|██████▍   | 60/93 [00:21<00:08,  3.98it/s]\n",
      "Loss: 3.22524:  66%|██████▌   | 61/93 [00:21<00:07,  4.37it/s]\n",
      "Loss: 5.78491:  66%|██████▌   | 61/93 [00:22<00:07,  4.37it/s]\n",
      "Loss: 5.78491:  67%|██████▋   | 62/93 [00:22<00:09,  3.23it/s]\n",
      "Loss: 4.70829:  67%|██████▋   | 62/93 [00:22<00:09,  3.23it/s]\n",
      "Loss: 4.70829:  68%|██████▊   | 63/93 [00:22<00:08,  3.71it/s]\n",
      "Loss: 3.55295:  68%|██████▊   | 63/93 [00:22<00:08,  3.71it/s]\n",
      "Loss: 3.55295:  69%|██████▉   | 64/93 [00:22<00:07,  4.08it/s]\n",
      "Loss: 5.77023:  69%|██████▉   | 64/93 [00:22<00:07,  4.08it/s]\n",
      "Loss: 5.77023:  70%|██████▉   | 65/93 [00:22<00:06,  4.45it/s]\n",
      "Loss: 4.45478:  70%|██████▉   | 65/93 [00:23<00:06,  4.45it/s]\n",
      "Loss: 4.45478:  71%|███████   | 66/93 [00:23<00:08,  3.37it/s]\n",
      "Loss: 3.27479:  71%|███████   | 66/93 [00:23<00:08,  3.37it/s]\n",
      "Loss: 3.27479:  72%|███████▏  | 67/93 [00:23<00:06,  3.84it/s]\n",
      "Loss: 6.07656:  72%|███████▏  | 67/93 [00:23<00:06,  3.84it/s]\n",
      "Loss: 6.07656:  73%|███████▎  | 68/93 [00:23<00:05,  4.27it/s]\n",
      "Loss: 4.25546:  73%|███████▎  | 68/93 [00:23<00:05,  4.27it/s]\n",
      "Loss: 4.25546:  74%|███████▍  | 69/93 [00:23<00:05,  4.60it/s]\n",
      "Loss: 2.62732:  74%|███████▍  | 69/93 [00:24<00:05,  4.60it/s]\n",
      "Loss: 2.62732:  75%|███████▌  | 70/93 [00:24<00:06,  3.41it/s]\n",
      "Loss: 4.39996:  75%|███████▌  | 70/93 [00:24<00:06,  3.41it/s]\n",
      "Loss: 4.39996:  76%|███████▋  | 71/93 [00:24<00:05,  3.90it/s]\n",
      "Loss: 4.70704:  76%|███████▋  | 71/93 [00:24<00:05,  3.90it/s]\n",
      "Loss: 4.70704:  77%|███████▋  | 72/93 [00:24<00:04,  4.36it/s]\n",
      "Loss: 2.66286:  77%|███████▋  | 72/93 [00:24<00:04,  4.36it/s]\n",
      "Loss: 2.66286:  78%|███████▊  | 73/93 [00:24<00:04,  4.74it/s]\n",
      "Loss: 4.00380:  78%|███████▊  | 73/93 [00:25<00:04,  4.74it/s]\n",
      "Loss: 4.00380:  80%|███████▉  | 74/93 [00:25<00:04,  3.90it/s]\n",
      "Loss: 7.26249:  80%|███████▉  | 74/93 [00:25<00:04,  3.90it/s]\n",
      "Loss: 7.26249:  81%|████████  | 75/93 [00:25<00:04,  4.34it/s]\n",
      "Loss: 4.49227:  81%|████████  | 75/93 [00:25<00:04,  4.34it/s]\n",
      "Loss: 4.49227:  82%|████████▏ | 76/93 [00:25<00:03,  4.73it/s]\n",
      "Loss: 2.48279:  82%|████████▏ | 76/93 [00:25<00:03,  4.73it/s]\n",
      "Loss: 2.48279:  83%|████████▎ | 77/93 [00:25<00:03,  5.00it/s]\n",
      "Loss: 1.52367:  83%|████████▎ | 77/93 [00:25<00:03,  5.00it/s]\n",
      "Loss: 1.52367:  84%|████████▍ | 78/93 [00:25<00:03,  3.94it/s]\n",
      "Loss: 0.47267:  84%|████████▍ | 78/93 [00:26<00:03,  3.94it/s]\n",
      "Loss: 0.47267:  85%|████████▍ | 79/93 [00:26<00:03,  4.37it/s]\n",
      "Loss: 2.24342:  85%|████████▍ | 79/93 [00:26<00:03,  4.37it/s]\n",
      "Loss: 2.24342:  86%|████████▌ | 80/93 [00:26<00:02,  4.76it/s]\n",
      "Loss: 1.08397:  86%|████████▌ | 80/93 [00:26<00:02,  4.76it/s]\n",
      "Loss: 1.08397:  87%|████████▋ | 81/93 [00:26<00:02,  5.08it/s]\n",
      "Loss: 2.87443:  87%|████████▋ | 81/93 [00:26<00:02,  5.08it/s]\n",
      "Loss: 2.87443:  88%|████████▊ | 82/93 [00:26<00:02,  4.37it/s]\n",
      "Loss: 1.32400:  88%|████████▊ | 82/93 [00:26<00:02,  4.37it/s]\n",
      "Loss: 1.32400:  89%|████████▉ | 83/93 [00:26<00:02,  4.73it/s]\n",
      "Loss: 1.65605:  89%|████████▉ | 83/93 [00:27<00:02,  4.73it/s]\n",
      "Loss: 1.65605:  90%|█████████ | 84/93 [00:27<00:01,  5.05it/s]\n",
      "Loss: 2.92768:  90%|█████████ | 84/93 [00:27<00:01,  5.05it/s]\n",
      "Loss: 2.92768:  91%|█████████▏| 85/93 [00:27<00:01,  5.31it/s]\n",
      "Loss: 0.87687:  91%|█████████▏| 85/93 [00:27<00:01,  5.31it/s]\n",
      "Loss: 0.87687:  92%|█████████▏| 86/93 [00:27<00:01,  3.63it/s]\n",
      "Loss: 0.00055:  92%|█████████▏| 86/93 [00:27<00:01,  3.63it/s]\n",
      "Loss: 0.00055:  94%|█████████▎| 87/93 [00:27<00:01,  4.08it/s]\n",
      "Loss: 0.82376:  94%|█████████▎| 87/93 [00:28<00:01,  4.08it/s]\n",
      "Loss: 0.82376:  95%|█████████▍| 88/93 [00:28<00:01,  4.47it/s]\n",
      "Loss: 1.89299:  95%|█████████▍| 88/93 [00:28<00:01,  4.47it/s]\n",
      "Loss: 1.89299:  96%|█████████▌| 89/93 [00:28<00:00,  4.83it/s]\n",
      "Loss: 1.46380:  96%|█████████▌| 89/93 [00:28<00:00,  4.83it/s]\n",
      "Loss: 1.46380:  97%|█████████▋| 90/93 [00:28<00:00,  4.14it/s]\n",
      "Loss: 0.99464:  97%|█████████▋| 90/93 [00:28<00:00,  4.14it/s]\n",
      "Loss: 0.99464:  98%|█████████▊| 91/93 [00:28<00:00,  4.59it/s]\n",
      "Loss: 2.67338:  98%|█████████▊| 91/93 [00:28<00:00,  4.59it/s]\n",
      "Loss: 2.67338:  99%|█████████▉| 92/93 [00:28<00:00,  4.96it/s]\n",
      "Loss: 3.53472:  99%|█████████▉| 92/93 [00:29<00:00,  4.96it/s]\n",
      "Loss: 3.53472: 100%|██████████| 93/93 [00:29<00:00,  5.26it/s]\n",
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\n",
      "  4%|▍         | 1/26 [00:06<02:30,  6.02s/it]\n",
      " 12%|█▏        | 3/26 [00:06<01:37,  4.23s/it]\n",
      " 19%|█▉        | 5/26 [00:06<01:03,  3.00s/it]\n",
      " 31%|███       | 8/26 [00:06<00:38,  2.12s/it]\n",
      " 35%|███▍      | 9/26 [00:06<00:26,  1.56s/it]\n",
      " 46%|████▌     | 12/26 [00:06<00:15,  1.11s/it]\n",
      " 54%|█████▍    | 14/26 [00:07<00:09,  1.21it/s]\n",
      " 65%|██████▌   | 17/26 [00:07<00:05,  1.63it/s]\n",
      " 73%|███████▎  | 19/26 [00:07<00:03,  2.25it/s]\n",
      " 81%|████████  | 21/26 [00:08<00:01,  2.70it/s]\n",
      " 92%|█████████▏| 24/26 [00:08<00:00,  3.67it/s]\n",
      "100%|██████████| 26/26 [00:08<00:00,  4.19it/s]\n",
      "\n",
      "  0%|          | 0/93 [00:00<?, ?it/s]\n",
      "Loss: 5.62513:   0%|          | 0/93 [00:06<?, ?it/s]\n",
      "Loss: 5.62513:   1%|          | 1/93 [00:06<10:16,  6.70s/it]\n",
      "Loss: 2.39312:   1%|          | 1/93 [00:06<10:16,  6.70s/it]\n",
      "Loss: 2.39312:   2%|▏         | 2/93 [00:06<07:11,  4.74s/it]\n",
      "Loss: 0.17422:   2%|▏         | 2/93 [00:07<07:11,  4.74s/it]\n",
      "Loss: 0.17422:   3%|▎         | 3/93 [00:07<05:03,  3.37s/it]\n",
      "Loss: 5.95306:   3%|▎         | 3/93 [00:07<05:03,  3.37s/it]\n",
      "Loss: 5.95306:   4%|▍         | 4/93 [00:07<03:34,  2.41s/it]\n",
      "Loss: 1.51310:   4%|▍         | 4/93 [00:07<03:34,  2.41s/it]\n",
      "Loss: 1.51310:   5%|▌         | 5/93 [00:07<02:39,  1.81s/it]\n",
      "Loss: 1.20944:   5%|▌         | 5/93 [00:07<02:39,  1.81s/it]\n",
      "Loss: 1.20944:   6%|▋         | 6/93 [00:07<01:54,  1.32s/it]\n",
      "Loss: 0.66098:   6%|▋         | 6/93 [00:07<01:54,  1.32s/it]\n",
      "Loss: 0.66098:   8%|▊         | 7/93 [00:07<01:23,  1.03it/s]\n",
      "Loss: 0.13350:   8%|▊         | 7/93 [00:08<01:23,  1.03it/s]\n",
      "Loss: 0.13350:   9%|▊         | 8/93 [00:08<01:02,  1.36it/s]\n",
      "Loss: 1.59941:   9%|▊         | 8/93 [00:08<01:02,  1.36it/s]\n",
      "Loss: 1.59941:  10%|▉         | 9/93 [00:08<00:49,  1.69it/s]\n",
      "Loss: 3.51888:  10%|▉         | 9/93 [00:08<00:49,  1.69it/s]\n",
      "Loss: 3.51888:  11%|█         | 10/93 [00:08<00:38,  2.14it/s]\n",
      "Loss: 2.83185:  11%|█         | 10/93 [00:08<00:38,  2.14it/s]\n",
      "Loss: 2.83185:  12%|█▏        | 11/93 [00:08<00:32,  2.54it/s]\n",
      "Loss: 3.30291:  12%|█▏        | 11/93 [00:08<00:32,  2.54it/s]\n",
      "Loss: 3.30291:  13%|█▎        | 12/93 [00:08<00:26,  3.02it/s]\n",
      "Loss: 3.88754:  13%|█▎        | 12/93 [00:09<00:26,  3.02it/s]\n",
      "Loss: 3.88754:  14%|█▍        | 13/93 [00:09<00:31,  2.52it/s]\n",
      "Loss: 0.54102:  14%|█▍        | 13/93 [00:09<00:31,  2.52it/s]\n",
      "Loss: 0.54102:  15%|█▌        | 14/93 [00:09<00:26,  3.04it/s]\n",
      "Loss: 1.56240:  15%|█▌        | 14/93 [00:09<00:26,  3.04it/s]\n",
      "Loss: 1.56240:  16%|█▌        | 15/93 [00:09<00:21,  3.56it/s]\n",
      "Loss: 1.20420:  16%|█▌        | 15/93 [00:10<00:21,  3.56it/s]\n",
      "Loss: 1.20420:  17%|█▋        | 16/93 [00:10<00:18,  4.06it/s]\n",
      "Loss: 1.42095:  17%|█▋        | 16/93 [00:10<00:18,  4.06it/s]\n",
      "Loss: 1.42095:  18%|█▊        | 17/93 [00:10<00:22,  3.41it/s]\n",
      "Loss: 0.89842:  18%|█▊        | 17/93 [00:10<00:22,  3.41it/s]\n",
      "Loss: 0.89842:  19%|█▉        | 18/93 [00:10<00:19,  3.88it/s]\n",
      "Loss: 0.91905:  19%|█▉        | 18/93 [00:10<00:19,  3.88it/s]\n",
      "Loss: 0.91905:  20%|██        | 19/93 [00:10<00:17,  4.29it/s]\n",
      "Loss: 0.51086:  20%|██        | 19/93 [00:10<00:17,  4.29it/s]\n",
      "Loss: 0.51086:  22%|██▏       | 20/93 [00:10<00:15,  4.67it/s]\n",
      "Loss: 0.47942:  22%|██▏       | 20/93 [00:11<00:15,  4.67it/s]\n",
      "Loss: 0.47942:  23%|██▎       | 21/93 [00:11<00:19,  3.62it/s]\n",
      "Loss: 0.65656:  23%|██▎       | 21/93 [00:11<00:19,  3.62it/s]\n",
      "Loss: 0.65656:  24%|██▎       | 22/93 [00:11<00:17,  4.04it/s]\n",
      "Loss: 3.85852:  24%|██▎       | 22/93 [00:11<00:17,  4.04it/s]\n",
      "Loss: 3.85852:  25%|██▍       | 23/93 [00:11<00:16,  4.32it/s]\n",
      "Loss: 0.00589:  25%|██▍       | 23/93 [00:11<00:16,  4.32it/s]\n",
      "Loss: 0.00589:  26%|██▌       | 24/93 [00:11<00:14,  4.68it/s]\n",
      "Loss: 0.02538:  26%|██▌       | 24/93 [00:12<00:14,  4.68it/s]\n",
      "Loss: 0.02538:  27%|██▋       | 25/93 [00:12<00:20,  3.34it/s]\n",
      "Loss: 1.21727:  27%|██▋       | 25/93 [00:12<00:20,  3.34it/s]\n",
      "Loss: 1.21727:  28%|██▊       | 26/93 [00:12<00:17,  3.84it/s]\n",
      "Loss: 2.64578:  28%|██▊       | 26/93 [00:12<00:17,  3.84it/s]\n",
      "Loss: 2.64578:  29%|██▉       | 27/93 [00:12<00:15,  4.29it/s]\n",
      "Loss: 0.64834:  29%|██▉       | 27/93 [00:12<00:15,  4.29it/s]\n",
      "Loss: 0.64834:  30%|███       | 28/93 [00:12<00:13,  4.68it/s]\n",
      "Loss: 2.83111:  30%|███       | 28/93 [00:13<00:13,  4.68it/s]\n",
      "Loss: 2.83111:  31%|███       | 29/93 [00:13<00:17,  3.58it/s]\n",
      "Loss: 0.70975:  31%|███       | 29/93 [00:13<00:17,  3.58it/s]\n",
      "Loss: 0.70975:  32%|███▏      | 30/93 [00:13<00:15,  4.05it/s]\n",
      "Loss: 0.84074:  32%|███▏      | 30/93 [00:13<00:15,  4.05it/s]\n",
      "Loss: 0.84074:  33%|███▎      | 31/93 [00:13<00:14,  4.43it/s]\n",
      "Loss: 5.79171:  33%|███▎      | 31/93 [00:13<00:14,  4.43it/s]\n",
      "Loss: 5.79171:  34%|███▍      | 32/93 [00:13<00:12,  4.76it/s]\n",
      "Loss: 1.36546:  34%|███▍      | 32/93 [00:14<00:12,  4.76it/s]\n",
      "Loss: 1.36546:  35%|███▌      | 33/93 [00:14<00:16,  3.69it/s]\n",
      "Loss: 1.22941:  35%|███▌      | 33/93 [00:14<00:16,  3.69it/s]\n",
      "Loss: 1.22941:  37%|███▋      | 34/93 [00:14<00:15,  3.93it/s]\n",
      "Loss: 1.35761:  37%|███▋      | 34/93 [00:14<00:15,  3.93it/s]\n",
      "Loss: 1.35761:  38%|███▊      | 35/93 [00:14<00:13,  4.38it/s]\n",
      "Loss: 1.21064:  38%|███▊      | 35/93 [00:14<00:13,  4.38it/s]\n",
      "Loss: 1.21064:  39%|███▊      | 36/93 [00:14<00:12,  4.75it/s]\n",
      "Loss: 0.96186:  39%|███▊      | 36/93 [00:15<00:12,  4.75it/s]\n",
      "Loss: 0.96186:  40%|███▉      | 37/93 [00:15<00:14,  3.83it/s]\n",
      "Loss: 2.21895:  40%|███▉      | 37/93 [00:15<00:14,  3.83it/s]\n",
      "Loss: 2.21895:  41%|████      | 38/93 [00:15<00:12,  4.30it/s]\n",
      "Loss: 1.13754:  41%|████      | 38/93 [00:15<00:12,  4.30it/s]\n",
      "Loss: 1.13754:  42%|████▏     | 39/93 [00:15<00:11,  4.69it/s]\n",
      "Loss: 1.71960:  42%|████▏     | 39/93 [00:15<00:11,  4.69it/s]\n",
      "Loss: 1.71960:  43%|████▎     | 40/93 [00:15<00:10,  4.93it/s]\n",
      "Loss: 1.48456:  43%|████▎     | 40/93 [00:16<00:10,  4.93it/s]\n",
      "Loss: 1.48456:  44%|████▍     | 41/93 [00:16<00:13,  3.82it/s]\n",
      "Loss: 1.00668:  44%|████▍     | 41/93 [00:16<00:13,  3.82it/s]\n",
      "Loss: 1.00668:  45%|████▌     | 42/93 [00:16<00:12,  4.22it/s]\n",
      "Loss: 2.15688:  45%|████▌     | 42/93 [00:16<00:12,  4.22it/s]\n",
      "Loss: 2.15688:  46%|████▌     | 43/93 [00:16<00:10,  4.61it/s]\n",
      "Loss: 0.01995:  46%|████▌     | 43/93 [00:16<00:10,  4.61it/s]\n",
      "Loss: 0.01995:  47%|████▋     | 44/93 [00:16<00:09,  4.95it/s]\n",
      "Loss: 2.18019:  47%|████▋     | 44/93 [00:17<00:09,  4.95it/s]\n",
      "Loss: 2.18019:  48%|████▊     | 45/93 [00:17<00:12,  3.93it/s]\n",
      "Loss: 2.86575:  48%|████▊     | 45/93 [00:17<00:12,  3.93it/s]\n",
      "Loss: 2.86575:  49%|████▉     | 46/93 [00:17<00:10,  4.39it/s]\n",
      "Loss: 2.02243:  49%|████▉     | 46/93 [00:17<00:10,  4.39it/s]\n",
      "Loss: 2.02243:  51%|█████     | 47/93 [00:17<00:09,  4.60it/s]\n",
      "Loss: 0.78179:  51%|█████     | 47/93 [00:17<00:09,  4.60it/s]\n",
      "Loss: 0.78179:  52%|█████▏    | 48/93 [00:17<00:09,  4.92it/s]\n",
      "Loss: 1.69594:  52%|█████▏    | 48/93 [00:17<00:09,  4.92it/s]\n",
      "Loss: 1.69594:  53%|█████▎    | 49/93 [00:17<00:11,  3.83it/s]\n",
      "Loss: 3.82902:  53%|█████▎    | 49/93 [00:18<00:11,  3.83it/s]\n",
      "Loss: 3.82902:  54%|█████▍    | 50/93 [00:18<00:10,  4.29it/s]\n",
      "Loss: 4.34514:  54%|█████▍    | 50/93 [00:18<00:10,  4.29it/s]\n",
      "Loss: 4.34514:  55%|█████▍    | 51/93 [00:18<00:09,  4.43it/s]\n",
      "Loss: 1.22549:  55%|█████▍    | 51/93 [00:18<00:09,  4.43it/s]\n",
      "Loss: 1.22549:  56%|█████▌    | 52/93 [00:18<00:08,  4.74it/s]\n",
      "Loss: 1.10847:  56%|█████▌    | 52/93 [00:18<00:08,  4.74it/s]\n",
      "Loss: 1.10847:  57%|█████▋    | 53/93 [00:18<00:09,  4.12it/s]\n",
      "Loss: 2.74415:  57%|█████▋    | 53/93 [00:19<00:09,  4.12it/s]\n",
      "Loss: 2.74415:  58%|█████▊    | 54/93 [00:19<00:08,  4.49it/s]\n",
      "Loss: 0.07985:  58%|█████▊    | 54/93 [00:19<00:08,  4.49it/s]\n",
      "Loss: 0.07985:  59%|█████▉    | 55/93 [00:19<00:08,  4.56it/s]\n",
      "Loss: 2.06102:  59%|█████▉    | 55/93 [00:19<00:08,  4.56it/s]\n",
      "Loss: 2.06102:  60%|██████    | 56/93 [00:19<00:07,  4.90it/s]\n",
      "Loss: 0.67798:  60%|██████    | 56/93 [00:19<00:07,  4.90it/s]\n",
      "Loss: 0.67798:  61%|██████▏   | 57/93 [00:19<00:08,  4.31it/s]\n",
      "Loss: 0.74446:  61%|██████▏   | 57/93 [00:20<00:08,  4.31it/s]\n",
      "Loss: 0.74446:  62%|██████▏   | 58/93 [00:20<00:09,  3.84it/s]\n",
      "Loss: 0.00325:  62%|██████▏   | 58/93 [00:20<00:09,  3.84it/s]\n",
      "Loss: 0.00325:  63%|██████▎   | 59/93 [00:20<00:08,  4.17it/s]\n",
      "Loss: 2.27513:  63%|██████▎   | 59/93 [00:20<00:08,  4.17it/s]\n",
      "Loss: 2.27513:  65%|██████▍   | 60/93 [00:20<00:07,  4.58it/s]\n",
      "Loss: 2.53623:  65%|██████▍   | 60/93 [00:20<00:07,  4.58it/s]\n",
      "Loss: 2.53623:  66%|██████▌   | 61/93 [00:20<00:07,  4.15it/s]\n",
      "Loss: 2.69232:  66%|██████▌   | 61/93 [00:20<00:07,  4.15it/s]\n",
      "Loss: 2.69232:  67%|██████▋   | 62/93 [00:20<00:07,  3.98it/s]\n",
      "Loss: 0.56658:  67%|██████▋   | 62/93 [00:21<00:07,  3.98it/s]\n",
      "Loss: 0.56658:  68%|██████▊   | 63/93 [00:21<00:07,  3.95it/s]\n",
      "Loss: 3.28089:  68%|██████▊   | 63/93 [00:21<00:07,  3.95it/s]\n",
      "Loss: 3.28089:  69%|██████▉   | 64/93 [00:21<00:06,  4.38it/s]\n",
      "Loss: 1.10492:  69%|██████▉   | 64/93 [00:21<00:06,  4.38it/s]\n",
      "Loss: 1.10492:  70%|██████▉   | 65/93 [00:21<00:06,  4.37it/s]\n",
      "Loss: 1.86660:  70%|██████▉   | 65/93 [00:21<00:06,  4.37it/s]\n",
      "Loss: 1.86660:  71%|███████   | 66/93 [00:21<00:07,  3.61it/s]\n",
      "Loss: 0.03628:  71%|███████   | 66/93 [00:22<00:07,  3.61it/s]\n",
      "Loss: 0.03628:  72%|███████▏  | 67/93 [00:22<00:06,  3.95it/s]\n",
      "Loss: 1.01414:  72%|███████▏  | 67/93 [00:22<00:06,  3.95it/s]\n",
      "Loss: 1.01414:  73%|███████▎  | 68/93 [00:22<00:05,  4.34it/s]\n",
      "Loss: 1.80051:  73%|███████▎  | 68/93 [00:22<00:05,  4.34it/s]\n",
      "Loss: 1.80051:  74%|███████▍  | 69/93 [00:22<00:05,  4.54it/s]\n",
      "Loss: 1.28711:  74%|███████▍  | 69/93 [00:22<00:05,  4.54it/s]\n",
      "Loss: 1.28711:  75%|███████▌  | 70/93 [00:22<00:05,  3.95it/s]\n",
      "Loss: 1.43726:  75%|███████▌  | 70/93 [00:23<00:05,  3.95it/s]\n",
      "Loss: 1.43726:  76%|███████▋  | 71/93 [00:23<00:05,  4.09it/s]\n",
      "Loss: 2.11003:  76%|███████▋  | 71/93 [00:23<00:05,  4.09it/s]\n",
      "Loss: 2.11003:  77%|███████▋  | 72/93 [00:23<00:04,  4.46it/s]\n",
      "Loss: 3.23106:  77%|███████▋  | 72/93 [00:23<00:04,  4.46it/s]\n",
      "Loss: 3.23106:  78%|███████▊  | 73/93 [00:23<00:04,  4.59it/s]\n",
      "Loss: 1.27196:  78%|███████▊  | 73/93 [00:23<00:04,  4.59it/s]\n",
      "Loss: 1.27196:  80%|███████▉  | 74/93 [00:23<00:04,  4.16it/s]\n",
      "Loss: 3.69954:  80%|███████▉  | 74/93 [00:24<00:04,  4.16it/s]\n",
      "Loss: 3.69954:  81%|████████  | 75/93 [00:24<00:04,  3.85it/s]\n",
      "Loss: 0.94118:  81%|████████  | 75/93 [00:24<00:04,  3.85it/s]\n",
      "Loss: 0.94118:  82%|████████▏ | 76/93 [00:24<00:03,  4.30it/s]\n",
      "Loss: 1.35635:  82%|████████▏ | 76/93 [00:24<00:03,  4.30it/s]\n",
      "Loss: 1.35635:  83%|████████▎ | 77/93 [00:24<00:03,  4.14it/s]\n",
      "Loss: 0.57041:  83%|████████▎ | 77/93 [00:24<00:03,  4.14it/s]\n",
      "Loss: 0.57041:  84%|████████▍ | 78/93 [00:24<00:03,  4.54it/s]\n",
      "Loss: 0.30118:  84%|████████▍ | 78/93 [00:25<00:03,  4.54it/s]\n",
      "Loss: 0.30118:  85%|████████▍ | 79/93 [00:25<00:03,  3.63it/s]\n",
      "Loss: 2.67306:  85%|████████▍ | 79/93 [00:25<00:03,  3.63it/s]\n",
      "Loss: 2.67306:  86%|████████▌ | 80/93 [00:25<00:03,  4.06it/s]\n",
      "Loss: 1.47523:  86%|████████▌ | 80/93 [00:25<00:03,  4.06it/s]\n",
      "Loss: 1.47523:  87%|████████▋ | 81/93 [00:25<00:02,  4.20it/s]\n",
      "Loss: 3.74653:  87%|████████▋ | 81/93 [00:25<00:02,  4.20it/s]\n",
      "Loss: 3.74653:  88%|████████▊ | 82/93 [00:25<00:02,  4.60it/s]\n",
      "Loss: 0.62884:  88%|████████▊ | 82/93 [00:26<00:02,  4.60it/s]\n",
      "Loss: 0.62884:  89%|████████▉ | 83/93 [00:26<00:02,  3.66it/s]\n",
      "Loss: 1.46536:  89%|████████▉ | 83/93 [00:26<00:02,  3.66it/s]\n",
      "Loss: 1.46536:  90%|█████████ | 84/93 [00:26<00:02,  4.12it/s]\n",
      "Loss: 1.95889:  90%|█████████ | 84/93 [00:26<00:02,  4.12it/s]\n",
      "Loss: 1.95889:  91%|█████████▏| 85/93 [00:26<00:02,  3.82it/s]\n",
      "Loss: 1.92883:  91%|█████████▏| 85/93 [00:26<00:02,  3.82it/s]\n",
      "Loss: 1.92883:  92%|█████████▏| 86/93 [00:26<00:01,  4.26it/s]\n",
      "Loss: 1.96425:  92%|█████████▏| 86/93 [00:27<00:01,  4.26it/s]\n",
      "Loss: 1.96425:  94%|█████████▎| 87/93 [00:27<00:01,  3.81it/s]\n",
      "Loss: 3.02618:  94%|█████████▎| 87/93 [00:27<00:01,  3.81it/s]\n",
      "Loss: 3.02618:  95%|█████████▍| 88/93 [00:27<00:01,  4.24it/s]\n",
      "Loss: 3.81364:  95%|█████████▍| 88/93 [00:27<00:01,  4.24it/s]\n",
      "Loss: 3.81364:  96%|█████████▌| 89/93 [00:27<00:01,  3.70it/s]\n",
      "Loss: 1.77393:  96%|█████████▌| 89/93 [00:27<00:01,  3.70it/s]\n",
      "Loss: 1.77393:  97%|█████████▋| 90/93 [00:27<00:00,  4.18it/s]\n",
      "Loss: 1.46465:  97%|█████████▋| 90/93 [00:27<00:00,  4.18it/s]\n",
      "Loss: 1.46465:  98%|█████████▊| 91/93 [00:27<00:00,  4.38it/s]\n",
      "Loss: 3.43391:  98%|█████████▊| 91/93 [00:28<00:00,  4.38it/s]\n",
      "Loss: 3.43391:  99%|█████████▉| 92/93 [00:28<00:00,  4.59it/s]\n",
      "Loss: 2.85615:  99%|█████████▉| 92/93 [00:28<00:00,  4.59it/s]\n",
      "Loss: 2.85615: 100%|██████████| 93/93 [00:28<00:00,  3.93it/s]\n",
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\n",
      "  4%|▍         | 1/26 [00:06<02:30,  6.03s/it]\n",
      " 12%|█▏        | 3/26 [00:06<01:37,  4.24s/it]\n",
      " 19%|█▉        | 5/26 [00:06<01:03,  3.01s/it]\n",
      " 31%|███       | 8/26 [00:06<00:38,  2.12s/it]\n",
      " 38%|███▊      | 10/26 [00:06<00:24,  1.53s/it]\n",
      " 50%|█████     | 13/26 [00:07<00:14,  1.11s/it]\n",
      " 62%|██████▏   | 16/26 [00:07<00:07,  1.26it/s]\n",
      " 69%|██████▉   | 18/26 [00:07<00:04,  1.66it/s]\n",
      " 81%|████████  | 21/26 [00:08<00:02,  2.17it/s]\n",
      " 92%|█████████▏| 24/26 [00:08<00:00,  2.97it/s]\n",
      "100%|██████████| 26/26 [00:08<00:00,  3.54it/s]\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\n",
      "  8%|▊         | 1/13 [00:06<01:13,  6.10s/it]\n",
      " 23%|██▎       | 3/13 [00:06<00:42,  4.29s/it]\n",
      " 38%|███▊      | 5/13 [00:06<00:24,  3.05s/it]\n",
      " 62%|██████▏   | 8/13 [00:06<00:10,  2.15s/it]\n",
      " 77%|███████▋  | 10/13 [00:06<00:04,  1.55s/it]\n",
      "100%|██████████| 13/13 [00:07<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "!python3 -u ../src/main.py --mode train --img-dir ../Data/landmark-recognition-2020 --log-dir ../logs_final/ --model-dir ../weights_final/ --name mason_4px_64b_02ep_2000lb --min-img-per-label 2000 --img-dim 4 --epochs 2 --batch-size 64\n",
    "!python3 -u ../src/main.py --mode train --img-dir ../Data/landmark-recognition-2020 --log-dir ../logs_final/ --model-dir ../weights_final/ --name mason_16px_64b_02ep_2000lb --min-img-per-label 2000 --img-dim 16 --epochs 2 --batch-size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 350/1378 [01:59<05:28,  3.13it/s]\n",
      "Loss: 33.42516:  25%|██▌       | 350/1378 [02:00<05:28,  3.13it/s]\n",
      "Loss: 33.42516:  25%|██▌       | 351/1378 [02:00<05:27,  3.13it/s]\n",
      "Loss: 35.05573:  25%|██▌       | 351/1378 [02:00<05:27,  3.13it/s]\n",
      "Loss: 35.05573:  26%|██▌       | 352/1378 [02:00<05:27,  3.14it/s]\n",
      "Loss: 36.83980:  26%|██▌       | 352/1378 [02:00<05:27,  3.14it/s]\n",
      "Loss: 36.83980:  26%|██▌       | 353/1378 [02:00<05:27,  3.13it/s]\n",
      "Loss: 34.78593:  26%|██▌       | 353/1378 [02:01<05:27,  3.13it/s]\n",
      "Loss: 34.78593:  26%|██▌       | 354/1378 [02:01<05:27,  3.13it/s]\n",
      "Loss: 32.58337:  26%|██▌       | 354/1378 [02:01<05:27,  3.13it/s]\n",
      "Loss: 32.58337:  26%|██▌       | 355/1378 [02:01<05:27,  3.12it/s]\n",
      "Loss: 32.69614:  26%|██▌       | 355/1378 [02:01<05:27,  3.12it/s]\n",
      "Loss: 32.69614:  26%|██▌       | 356/1378 [02:01<05:26,  3.13it/s]\n",
      "Loss: 31.29003:  26%|██▌       | 356/1378 [02:02<05:26,  3.13it/s]\n",
      "Loss: 31.29003:  26%|██▌       | 357/1378 [02:02<05:26,  3.13it/s]\n",
      "Loss: 35.95533:  26%|██▌       | 357/1378 [02:02<05:26,  3.13it/s]\n",
      "Loss: 35.95533:  26%|██▌       | 358/1378 [02:02<05:25,  3.13it/s]\n",
      "Loss: 36.81665:  26%|██▌       | 358/1378 [02:02<05:25,  3.13it/s]\n",
      "Loss: 36.81665:  26%|██▌       | 359/1378 [02:02<05:25,  3.13it/s]\n",
      "Loss: 34.62583:  26%|██▌       | 359/1378 [02:03<05:25,  3.13it/s]\n",
      "Loss: 34.62583:  26%|██▌       | 360/1378 [02:03<05:25,  3.13it/s]\n",
      "Loss: 34.66364:  26%|██▌       | 360/1378 [02:03<05:25,  3.13it/s]\n",
      "Loss: 34.66364:  26%|██▌       | 361/1378 [02:03<05:24,  3.13it/s]\n",
      "Loss: 33.74405:  26%|██▌       | 361/1378 [02:03<05:24,  3.13it/s]\n",
      "Loss: 33.74405:  26%|██▋       | 362/1378 [02:03<05:24,  3.13it/s]\n",
      "Loss: 34.81194:  26%|██▋       | 362/1378 [02:04<05:24,  3.13it/s]\n",
      "Loss: 34.81194:  26%|██▋       | 363/1378 [02:04<05:25,  3.12it/s]\n",
      "Loss: 35.67454:  26%|██▋       | 363/1378 [02:04<05:25,  3.12it/s]\n",
      "Loss: 35.67454:  26%|██▋       | 364/1378 [02:04<05:25,  3.12it/s]\n",
      "Loss: 31.76213:  26%|██▋       | 364/1378 [02:04<05:25,  3.12it/s]\n",
      "Loss: 31.76213:  26%|██▋       | 365/1378 [02:04<05:25,  3.12it/s]\n",
      "Loss: 35.00370:  26%|██▋       | 365/1378 [02:05<05:25,  3.12it/s]\n",
      "Loss: 35.00370:  27%|██▋       | 366/1378 [02:05<05:24,  3.12it/s]\n",
      "Loss: 34.60582:  27%|██▋       | 366/1378 [02:05<05:24,  3.12it/s]\n",
      "Loss: 34.60582:  27%|██▋       | 367/1378 [02:05<05:23,  3.13it/s]\n",
      "Loss: 35.02844:  27%|██▋       | 367/1378 [02:05<05:23,  3.13it/s]\n",
      "Loss: 35.02844:  27%|██▋       | 368/1378 [02:05<05:24,  3.12it/s]\n",
      "Loss: 34.51583:  27%|██▋       | 368/1378 [02:06<05:24,  3.12it/s]\n",
      "Loss: 34.51583:  27%|██▋       | 369/1378 [02:06<05:23,  3.12it/s]\n",
      "Loss: 33.94241:  27%|██▋       | 369/1378 [02:06<05:23,  3.12it/s]\n",
      "Loss: 33.94241:  27%|██▋       | 370/1378 [02:06<05:22,  3.12it/s]\n",
      "Loss: 33.75899:  27%|██▋       | 370/1378 [02:06<05:22,  3.12it/s]\n",
      "Loss: 33.75899:  27%|██▋       | 371/1378 [02:06<05:21,  3.13it/s]\n",
      "Loss: 33.68488:  27%|██▋       | 371/1378 [02:07<05:21,  3.13it/s]\n",
      "Loss: 33.68488:  27%|██▋       | 372/1378 [02:07<05:21,  3.12it/s]\n",
      "Loss: 32.57829:  27%|██▋       | 372/1378 [02:07<05:21,  3.12it/s]\n",
      "Loss: 32.57829:  27%|██▋       | 373/1378 [02:07<05:22,  3.12it/s]\n",
      "Loss: 33.69431:  27%|██▋       | 373/1378 [02:07<05:22,  3.12it/s]\n",
      "Loss: 33.69431:  27%|██▋       | 374/1378 [02:07<05:21,  3.12it/s]\n",
      "Loss: 35.73548:  27%|██▋       | 374/1378 [02:07<05:21,  3.12it/s]\n",
      "Loss: 35.73548:  27%|██▋       | 375/1378 [02:07<05:21,  3.12it/s]\n",
      "Loss: 34.56589:  27%|██▋       | 375/1378 [02:08<05:21,  3.12it/s]\n",
      "Loss: 34.56589:  27%|██▋       | 376/1378 [02:08<05:20,  3.13it/s]\n",
      "Loss: 32.87432:  27%|██▋       | 376/1378 [02:08<05:20,  3.13it/s]\n",
      "Loss: 32.87432:  27%|██▋       | 377/1378 [02:08<05:20,  3.13it/s]\n",
      "Loss: 33.28294:  27%|██▋       | 377/1378 [02:08<05:20,  3.13it/s]\n",
      "Loss: 33.28294:  27%|██▋       | 378/1378 [02:08<05:19,  3.13it/s]\n",
      "Loss: 33.28470:  27%|██▋       | 378/1378 [02:09<05:19,  3.13it/s]\n",
      "Loss: 33.28470:  28%|██▊       | 379/1378 [02:09<05:18,  3.13it/s]\n",
      "Loss: 33.04909:  28%|██▊       | 379/1378 [02:09<05:18,  3.13it/s]\n",
      "Loss: 33.04909:  28%|██▊       | 380/1378 [02:09<05:18,  3.13it/s]\n",
      "Loss: 34.35873:  28%|██▊       | 380/1378 [02:09<05:18,  3.13it/s]\n",
      "Loss: 34.35873:  28%|██▊       | 381/1378 [02:09<05:18,  3.13it/s]\n",
      "Loss: 33.14396:  28%|██▊       | 381/1378 [02:10<05:18,  3.13it/s]\n",
      "Loss: 33.14396:  28%|██▊       | 382/1378 [02:10<05:17,  3.14it/s]\n",
      "Loss: 35.25623:  28%|██▊       | 382/1378 [02:10<05:17,  3.14it/s]\n",
      "Loss: 35.25623:  28%|██▊       | 383/1378 [02:10<05:17,  3.14it/s]\n",
      "Loss: 34.61562:  28%|██▊       | 383/1378 [02:10<05:17,  3.14it/s]\n",
      "Loss: 34.61562:  28%|██▊       | 384/1378 [02:10<05:17,  3.13it/s]\n",
      "Loss: 35.42525:  28%|██▊       | 384/1378 [02:11<05:17,  3.13it/s]\n",
      "Loss: 35.42525:  28%|██▊       | 385/1378 [02:11<05:16,  3.13it/s]\n",
      "Loss: 34.85151:  28%|██▊       | 385/1378 [02:11<05:16,  3.13it/s]\n",
      "Loss: 34.85151:  28%|██▊       | 386/1378 [02:11<05:17,  3.13it/s]\n",
      "Loss: 33.74520:  28%|██▊       | 386/1378 [02:11<05:17,  3.13it/s]\n",
      "Loss: 33.74520:  28%|██▊       | 387/1378 [02:11<05:16,  3.13it/s]\n",
      "Loss: 35.14778:  28%|██▊       | 387/1378 [02:12<05:16,  3.13it/s]\n",
      "Loss: 35.14778:  28%|██▊       | 388/1378 [02:12<05:16,  3.13it/s]\n",
      "Loss: 31.91401:  28%|██▊       | 388/1378 [02:12<05:16,  3.13it/s]\n",
      "Loss: 31.91401:  28%|██▊       | 389/1378 [02:12<05:16,  3.13it/s]\n",
      "Loss: 32.91227:  28%|██▊       | 389/1378 [02:12<05:16,  3.13it/s]\n",
      "Loss: 32.91227:  28%|██▊       | 390/1378 [02:12<05:15,  3.13it/s]\n",
      "Loss: 33.85489:  28%|██▊       | 390/1378 [02:13<05:15,  3.13it/s]\n",
      "Loss: 33.85489:  28%|██▊       | 391/1378 [02:13<05:15,  3.12it/s]\n",
      "Loss: 35.21798:  28%|██▊       | 391/1378 [02:13<05:15,  3.12it/s]\n",
      "Loss: 35.21798:  28%|██▊       | 392/1378 [02:13<05:15,  3.12it/s]\n",
      "Loss: 34.28085:  28%|██▊       | 392/1378 [02:13<05:15,  3.12it/s]\n",
      "Loss: 34.28085:  29%|██▊       | 393/1378 [02:13<05:15,  3.12it/s]\n",
      "Loss: 37.10630:  29%|██▊       | 393/1378 [02:14<05:15,  3.12it/s]\n",
      "Loss: 37.10630:  29%|██▊       | 394/1378 [02:14<05:15,  3.12it/s]\n",
      "Loss: 34.80417:  29%|██▊       | 394/1378 [02:14<05:15,  3.12it/s]\n",
      "Loss: 34.80417:  29%|██▊       | 395/1378 [02:14<05:14,  3.13it/s]\n",
      "Loss: 35.55801:  29%|██▊       | 395/1378 [02:14<05:14,  3.13it/s]\n",
      "Loss: 35.55801:  29%|██▊       | 396/1378 [02:14<05:14,  3.13it/s]\n",
      "Loss: 33.03213:  29%|██▊       | 396/1378 [02:15<05:14,  3.13it/s]\n",
      "Loss: 33.03213:  29%|██▉       | 397/1378 [02:15<05:13,  3.13it/s]\n",
      "Loss: 33.28133:  29%|██▉       | 397/1378 [02:15<05:13,  3.13it/s]\n",
      "Loss: 33.28133:  29%|██▉       | 398/1378 [02:15<05:12,  3.13it/s]\n",
      "Loss: 32.62409:  29%|██▉       | 398/1378 [02:15<05:12,  3.13it/s]\n",
      "Loss: 32.62409:  29%|██▉       | 399/1378 [02:15<05:12,  3.13it/s]\n",
      "Loss: 32.98005:  29%|██▉       | 399/1378 [02:15<05:12,  3.13it/s]\n",
      "Loss: 32.98005:  29%|██▉       | 400/1378 [02:15<05:12,  3.13it/s]\n",
      "Loss: 35.14143:  29%|██▉       | 400/1378 [02:16<05:12,  3.13it/s]\n",
      "Loss: 35.14143:  29%|██▉       | 401/1378 [02:16<05:11,  3.14it/s]\n",
      "Loss: 36.11056:  29%|██▉       | 401/1378 [02:16<05:11,  3.14it/s]\n",
      "Loss: 36.11056:  29%|██▉       | 402/1378 [02:16<05:12,  3.13it/s]\n",
      "Loss: 34.82299:  29%|██▉       | 402/1378 [02:16<05:12,  3.13it/s]\n",
      "Loss: 34.82299:  29%|██▉       | 403/1378 [02:16<05:11,  3.13it/s]\n",
      "Loss: 35.64275:  29%|██▉       | 403/1378 [02:17<05:11,  3.13it/s]\n",
      "Loss: 35.64275:  29%|██▉       | 404/1378 [02:17<05:11,  3.13it/s]\n",
      "Loss: 31.20388:  29%|██▉       | 404/1378 [02:17<05:11,  3.13it/s]\n",
      "Loss: 31.20388:  29%|██▉       | 405/1378 [02:17<05:10,  3.13it/s]\n",
      "Loss: 35.29383:  29%|██▉       | 405/1378 [02:17<05:10,  3.13it/s]\n",
      "Loss: 35.29383:  29%|██▉       | 406/1378 [02:17<05:10,  3.13it/s]\n",
      "Loss: 34.84373:  29%|██▉       | 406/1378 [02:18<05:10,  3.13it/s]\n",
      "Loss: 34.84373:  30%|██▉       | 407/1378 [02:18<05:10,  3.13it/s]\n",
      "Loss: 33.02353:  30%|██▉       | 407/1378 [02:18<05:10,  3.13it/s]\n",
      "Loss: 33.02353:  30%|██▉       | 408/1378 [02:18<05:10,  3.12it/s]\n",
      "Loss: 34.23952:  30%|██▉       | 408/1378 [02:18<05:10,  3.12it/s]\n",
      "Loss: 34.23952:  30%|██▉       | 409/1378 [02:18<05:09,  3.13it/s]\n",
      "Loss: 32.40537:  30%|██▉       | 409/1378 [02:19<05:09,  3.13it/s]\n",
      "Loss: 32.40537:  30%|██▉       | 410/1378 [02:19<05:09,  3.13it/s]\n",
      "Loss: 34.63020:  30%|██▉       | 410/1378 [02:19<05:09,  3.13it/s]\n",
      "Loss: 34.63020:  30%|██▉       | 411/1378 [02:19<05:09,  3.12it/s]\n",
      "Loss: 32.93338:  30%|██▉       | 411/1378 [02:19<05:09,  3.12it/s]\n",
      "Loss: 32.93338:  30%|██▉       | 412/1378 [02:19<05:09,  3.12it/s]\n",
      "Loss: 35.22945:  30%|██▉       | 412/1378 [02:20<05:09,  3.12it/s]\n",
      "Loss: 35.22945:  30%|██▉       | 413/1378 [02:20<05:09,  3.12it/s]\n",
      "Loss: 33.62389:  30%|██▉       | 413/1378 [02:20<05:09,  3.12it/s]\n",
      "Loss: 33.62389:  30%|███       | 414/1378 [02:20<05:08,  3.12it/s]\n",
      "Loss: 32.80511:  30%|███       | 414/1378 [02:20<05:08,  3.12it/s]\n",
      "Loss: 32.80511:  30%|███       | 415/1378 [02:20<05:08,  3.12it/s]\n",
      "Loss: 35.41291:  30%|███       | 415/1378 [02:21<05:08,  3.12it/s]\n",
      "Loss: 35.41291:  30%|███       | 416/1378 [02:21<05:07,  3.12it/s]\n",
      "Loss: 36.01859:  30%|███       | 416/1378 [02:21<05:07,  3.12it/s]\n",
      "Loss: 36.01859:  30%|███       | 417/1378 [02:21<05:07,  3.13it/s]\n",
      "Loss: 31.17754:  30%|███       | 417/1378 [02:21<05:07,  3.13it/s]\n",
      "Loss: 31.17754:  30%|███       | 418/1378 [02:21<05:07,  3.12it/s]\n",
      "Loss: 34.89391:  30%|███       | 418/1378 [02:22<05:07,  3.12it/s]\n",
      "Loss: 34.89391:  30%|███       | 419/1378 [02:22<05:06,  3.13it/s]\n",
      "Loss: 34.95459:  30%|███       | 419/1378 [02:22<05:06,  3.13it/s]\n",
      "Loss: 34.95459:  30%|███       | 420/1378 [02:22<05:05,  3.13it/s]\n",
      "Loss: 35.68190:  30%|███       | 420/1378 [02:22<05:05,  3.13it/s]\n",
      "Loss: 35.68190:  31%|███       | 421/1378 [02:22<05:05,  3.13it/s]\n",
      "Loss: 33.00313:  31%|███       | 421/1378 [02:23<05:05,  3.13it/s]\n",
      "Loss: 33.00313:  31%|███       | 422/1378 [02:23<05:05,  3.13it/s]\n",
      "Loss: 36.55732:  31%|███       | 422/1378 [02:23<05:05,  3.13it/s]\n",
      "Loss: 36.55732:  31%|███       | 423/1378 [02:23<05:05,  3.13it/s]\n",
      "Loss: 34.10125:  31%|███       | 423/1378 [02:23<05:05,  3.13it/s]\n",
      "Loss: 34.10125:  31%|███       | 424/1378 [02:23<05:05,  3.12it/s]\n",
      "Loss: 33.84184:  31%|███       | 424/1378 [02:23<05:05,  3.12it/s]\n",
      "Loss: 33.84184:  31%|███       | 425/1378 [02:23<05:05,  3.12it/s]\n",
      "Loss: 33.48792:  31%|███       | 425/1378 [02:24<05:05,  3.12it/s]\n",
      "Loss: 33.48792:  31%|███       | 426/1378 [02:24<05:04,  3.12it/s]\n",
      "Loss: 35.35097:  31%|███       | 426/1378 [02:24<05:04,  3.12it/s]\n",
      "Loss: 35.35097:  31%|███       | 427/1378 [02:24<05:03,  3.13it/s]\n",
      "Loss: 33.95822:  31%|███       | 427/1378 [02:24<05:03,  3.13it/s]\n",
      "Loss: 33.95822:  31%|███       | 428/1378 [02:24<05:03,  3.13it/s]\n",
      "Loss: 34.72004:  31%|███       | 428/1378 [02:25<05:03,  3.13it/s]\n",
      "Loss: 34.72004:  31%|███       | 429/1378 [02:25<05:03,  3.12it/s]\n",
      "Loss: 34.17995:  31%|███       | 429/1378 [02:25<05:03,  3.12it/s]\n",
      "Loss: 34.17995:  31%|███       | 430/1378 [02:25<05:03,  3.12it/s]\n",
      "Loss: 34.35299:  31%|███       | 430/1378 [02:25<05:03,  3.12it/s]\n",
      "Loss: 34.35299:  31%|███▏      | 431/1378 [02:25<05:02,  3.13it/s]\n",
      "Loss: 34.86109:  31%|███▏      | 431/1378 [02:26<05:02,  3.13it/s]\n",
      "Loss: 34.86109:  31%|███▏      | 432/1378 [02:26<05:02,  3.13it/s]\n",
      "Loss: 36.03516:  31%|███▏      | 432/1378 [02:26<05:02,  3.13it/s]\n",
      "Loss: 36.03516:  31%|███▏      | 433/1378 [02:26<05:02,  3.12it/s]\n",
      "Loss: 34.43570:  31%|███▏      | 433/1378 [02:26<05:02,  3.12it/s]\n",
      "Loss: 34.43570:  31%|███▏      | 434/1378 [02:26<05:01,  3.13it/s]\n",
      "Loss: 33.91632:  31%|███▏      | 434/1378 [02:27<05:01,  3.13it/s]\n",
      "Loss: 33.91632:  32%|███▏      | 435/1378 [02:27<05:01,  3.13it/s]\n",
      "Loss: 34.33143:  32%|███▏      | 435/1378 [02:27<05:01,  3.13it/s]\n",
      "Loss: 34.33143:  32%|███▏      | 436/1378 [02:27<05:00,  3.13it/s]\n",
      "Loss: 35.14283:  32%|███▏      | 436/1378 [02:27<05:00,  3.13it/s]\n",
      "Loss: 35.14283:  32%|███▏      | 437/1378 [02:27<05:00,  3.13it/s]\n",
      "Loss: 36.66916:  32%|███▏      | 437/1378 [02:28<05:00,  3.13it/s]\n",
      "Loss: 36.66916:  32%|███▏      | 438/1378 [02:28<04:59,  3.13it/s]\n",
      "Loss: 32.73580:  32%|███▏      | 438/1378 [02:28<04:59,  3.13it/s]\n",
      "Loss: 32.73580:  32%|███▏      | 439/1378 [02:28<04:59,  3.13it/s]\n",
      "Loss: 33.35098:  32%|███▏      | 439/1378 [02:28<04:59,  3.13it/s]\n",
      "Loss: 33.35098:  32%|███▏      | 440/1378 [02:28<05:00,  3.12it/s]\n",
      "Loss: 34.44454:  32%|███▏      | 440/1378 [02:29<05:00,  3.12it/s]\n",
      "Loss: 34.44454:  32%|███▏      | 441/1378 [02:29<04:59,  3.13it/s]\n",
      "Loss: 33.53313:  32%|███▏      | 441/1378 [02:29<04:59,  3.13it/s]\n",
      "Loss: 33.53313:  32%|███▏      | 442/1378 [02:29<04:59,  3.13it/s]\n",
      "Loss: 33.60099:  32%|███▏      | 442/1378 [02:29<04:59,  3.13it/s]\n",
      "Loss: 33.60099:  32%|███▏      | 443/1378 [02:29<04:59,  3.12it/s]\n",
      "Loss: 33.77356:  32%|███▏      | 443/1378 [02:30<04:59,  3.12it/s]\n",
      "Loss: 33.77356:  32%|███▏      | 444/1378 [02:30<04:59,  3.12it/s]\n",
      "Loss: 33.35894:  32%|███▏      | 444/1378 [02:30<04:59,  3.12it/s]\n",
      "Loss: 33.35894:  32%|███▏      | 445/1378 [02:30<04:58,  3.13it/s]\n",
      "Loss: 34.44490:  32%|███▏      | 445/1378 [02:30<04:58,  3.13it/s]\n",
      "Loss: 34.44490:  32%|███▏      | 446/1378 [02:30<04:58,  3.12it/s]\n",
      "Loss: 33.84998:  32%|███▏      | 446/1378 [02:31<04:58,  3.12it/s]\n",
      "Loss: 33.84998:  32%|███▏      | 447/1378 [02:31<04:58,  3.12it/s]\n",
      "Loss: 33.45450:  32%|███▏      | 447/1378 [02:31<04:58,  3.12it/s]\n",
      "Loss: 33.45450:  33%|███▎      | 448/1378 [02:31<04:57,  3.12it/s]\n",
      "Loss: 33.37289:  33%|███▎      | 448/1378 [02:31<04:57,  3.12it/s]\n",
      "Loss: 33.37289:  33%|███▎      | 449/1378 [02:31<04:56,  3.13it/s]\n",
      "Loss: 33.20564:  33%|███▎      | 449/1378 [02:31<04:56,  3.13it/s]\n",
      "Loss: 33.20564:  33%|███▎      | 450/1378 [02:31<04:56,  3.13it/s]\n",
      "Loss: 33.69920:  33%|███▎      | 450/1378 [02:32<04:56,  3.13it/s]\n",
      "Loss: 33.69920:  33%|███▎      | 451/1378 [02:32<04:56,  3.13it/s]\n",
      "Loss: 34.32116:  33%|███▎      | 451/1378 [02:32<04:56,  3.13it/s]\n",
      "Loss: 34.32116:  33%|███▎      | 452/1378 [02:32<04:55,  3.13it/s]\n",
      "Loss: 33.98551:  33%|███▎      | 452/1378 [02:32<04:55,  3.13it/s]\n",
      "Loss: 33.98551:  33%|███▎      | 453/1378 [02:32<04:54,  3.14it/s]\n",
      "Loss: 33.06832:  33%|███▎      | 453/1378 [02:33<04:54,  3.14it/s]\n",
      "Loss: 33.06832:  33%|███▎      | 454/1378 [02:33<04:54,  3.14it/s]\n",
      "Loss: 33.76266:  33%|███▎      | 454/1378 [02:33<04:54,  3.14it/s]\n",
      "Loss: 33.76266:  33%|███▎      | 455/1378 [02:33<04:53,  3.14it/s]\n",
      "Loss: 32.77226:  33%|███▎      | 455/1378 [02:33<04:53,  3.14it/s]\n",
      "Loss: 32.77226:  33%|███▎      | 456/1378 [02:33<04:53,  3.14it/s]\n",
      "Loss: 31.85455:  33%|███▎      | 456/1378 [02:34<04:53,  3.14it/s]\n",
      "Loss: 31.85455:  33%|███▎      | 457/1378 [02:34<04:53,  3.14it/s]\n",
      "Loss: 31.17326:  33%|███▎      | 457/1378 [02:34<04:53,  3.14it/s]\n",
      "Loss: 31.17326:  33%|███▎      | 458/1378 [02:34<04:53,  3.13it/s]\n",
      "Loss: 33.24186:  33%|███▎      | 458/1378 [02:34<04:53,  3.13it/s]\n",
      "Loss: 33.24186:  33%|███▎      | 459/1378 [02:34<04:53,  3.13it/s]\n",
      "Loss: 35.04662:  33%|███▎      | 459/1378 [02:35<04:53,  3.13it/s]\n",
      "Loss: 35.04662:  33%|███▎      | 460/1378 [02:35<04:53,  3.13it/s]\n",
      "Loss: 33.54911:  33%|███▎      | 460/1378 [02:35<04:53,  3.13it/s]\n",
      "Loss: 33.54911:  33%|███▎      | 461/1378 [02:35<04:53,  3.12it/s]\n",
      "Loss: 33.27825:  33%|███▎      | 461/1378 [02:35<04:53,  3.12it/s]\n",
      "Loss: 33.27825:  34%|███▎      | 462/1378 [02:35<04:52,  3.13it/s]\n",
      "Loss: 35.27564:  34%|███▎      | 462/1378 [02:36<04:52,  3.13it/s]\n",
      "Loss: 35.27564:  34%|███▎      | 463/1378 [02:36<04:52,  3.13it/s]\n",
      "Loss: 33.40084:  34%|███▎      | 463/1378 [02:36<04:52,  3.13it/s]\n",
      "Loss: 33.40084:  34%|███▎      | 464/1378 [02:36<04:51,  3.13it/s]\n",
      "Loss: 35.55116:  34%|███▎      | 464/1378 [02:36<04:51,  3.13it/s]\n",
      "Loss: 35.55116:  34%|███▎      | 465/1378 [02:36<04:50,  3.14it/s]\n",
      "Loss: 34.16985:  34%|███▎      | 465/1378 [02:37<04:50,  3.14it/s]\n",
      "Loss: 34.16985:  34%|███▍      | 466/1378 [02:37<04:50,  3.14it/s]\n",
      "Loss: 33.38666:  34%|███▍      | 466/1378 [02:37<04:50,  3.14it/s]\n",
      "Loss: 33.38666:  34%|███▍      | 467/1378 [02:37<04:50,  3.14it/s]\n",
      "Loss: 35.40652:  34%|███▍      | 467/1378 [02:37<04:50,  3.14it/s]\n",
      "Loss: 35.40652:  34%|███▍      | 468/1378 [02:37<04:49,  3.14it/s]\n",
      "Loss: 32.96276:  34%|███▍      | 468/1378 [02:38<04:49,  3.14it/s]\n",
      "Loss: 32.96276:  34%|███▍      | 469/1378 [02:38<04:49,  3.14it/s]\n",
      "Loss: 35.99284:  34%|███▍      | 469/1378 [02:38<04:49,  3.14it/s]\n",
      "Loss: 35.99284:  34%|███▍      | 470/1378 [02:38<04:49,  3.14it/s]\n",
      "Loss: 33.40312:  34%|███▍      | 470/1378 [02:38<04:49,  3.14it/s]\n",
      "Loss: 33.40312:  34%|███▍      | 471/1378 [02:38<04:49,  3.14it/s]\n",
      "Loss: 32.81352:  34%|███▍      | 471/1378 [02:38<04:49,  3.14it/s]\n",
      "Loss: 32.81352:  34%|███▍      | 472/1378 [02:38<04:48,  3.14it/s]\n",
      "Loss: 33.32832:  34%|███▍      | 472/1378 [02:39<04:48,  3.14it/s]\n",
      "Loss: 33.32832:  34%|███▍      | 473/1378 [02:39<04:49,  3.13it/s]\n",
      "Loss: 33.09694:  34%|███▍      | 473/1378 [02:39<04:49,  3.13it/s]\n",
      "Loss: 33.09694:  34%|███▍      | 474/1378 [02:39<04:48,  3.13it/s]\n",
      "Loss: 33.82716:  34%|███▍      | 474/1378 [02:39<04:48,  3.13it/s]\n",
      "Loss: 33.82716:  34%|███▍      | 475/1378 [02:39<04:48,  3.13it/s]\n",
      "Loss: 33.89209:  34%|███▍      | 475/1378 [02:40<04:48,  3.13it/s]\n",
      "Loss: 33.89209:  35%|███▍      | 476/1378 [02:40<04:47,  3.14it/s]\n",
      "Loss: 33.28601:  35%|███▍      | 476/1378 [02:40<04:47,  3.14it/s]\n",
      "Loss: 33.28601:  35%|███▍      | 477/1378 [02:40<04:47,  3.13it/s]\n",
      "Loss: 33.85468:  35%|███▍      | 477/1378 [02:40<04:47,  3.13it/s]\n",
      "Loss: 33.85468:  35%|███▍      | 478/1378 [02:40<04:47,  3.13it/s]\n",
      "Loss: 33.27127:  35%|███▍      | 478/1378 [02:41<04:47,  3.13it/s]\n",
      "Loss: 33.27127:  35%|███▍      | 479/1378 [02:41<04:46,  3.13it/s]\n",
      "Loss: 35.29360:  35%|███▍      | 479/1378 [02:41<04:46,  3.13it/s]\n",
      "Loss: 35.29360:  35%|███▍      | 480/1378 [02:41<04:46,  3.13it/s]\n",
      "Loss: 34.64278:  35%|███▍      | 480/1378 [02:41<04:46,  3.13it/s]\n",
      "Loss: 34.64278:  35%|███▍      | 481/1378 [02:41<04:46,  3.13it/s]\n",
      "Loss: 34.82681:  35%|███▍      | 481/1378 [02:42<04:46,  3.13it/s]\n",
      "Loss: 34.82681:  35%|███▍      | 482/1378 [02:42<04:46,  3.13it/s]\n",
      "Loss: 31.47736:  35%|███▍      | 482/1378 [02:42<04:46,  3.13it/s]\n",
      "Loss: 31.47736:  35%|███▌      | 483/1378 [02:42<04:45,  3.13it/s]\n",
      "Loss: 32.86421:  35%|███▌      | 483/1378 [02:42<04:45,  3.13it/s]\n",
      "Loss: 32.86421:  35%|███▌      | 484/1378 [02:42<04:45,  3.14it/s]\n",
      "Loss: 36.34322:  35%|███▌      | 484/1378 [02:43<04:45,  3.14it/s]\n",
      "Loss: 36.34322:  35%|███▌      | 485/1378 [02:43<04:44,  3.14it/s]\n",
      "Loss: 35.25018:  35%|███▌      | 485/1378 [02:43<04:44,  3.14it/s]\n",
      "Loss: 35.25018:  35%|███▌      | 486/1378 [02:43<04:44,  3.13it/s]\n",
      "Loss: 33.35422:  35%|███▌      | 486/1378 [02:43<04:44,  3.13it/s]\n",
      "Loss: 33.35422:  35%|███▌      | 487/1378 [02:43<04:44,  3.14it/s]\n",
      "Loss: 32.53609:  35%|███▌      | 487/1378 [02:44<04:44,  3.14it/s]\n",
      "Loss: 32.53609:  35%|███▌      | 488/1378 [02:44<04:43,  3.14it/s]\n",
      "Loss: 35.55665:  35%|███▌      | 488/1378 [02:44<04:43,  3.14it/s]\n",
      "Loss: 35.55665:  35%|███▌      | 489/1378 [02:44<04:44,  3.13it/s]\n",
      "Loss: 34.93077:  35%|███▌      | 489/1378 [02:44<04:44,  3.13it/s]\n",
      "Loss: 34.93077:  36%|███▌      | 490/1378 [02:44<04:43,  3.13it/s]\n",
      "Loss: 35.38942:  36%|███▌      | 490/1378 [02:45<04:43,  3.13it/s]\n",
      "Loss: 35.38942:  36%|███▌      | 491/1378 [02:45<04:43,  3.13it/s]\n",
      "Loss: 35.54962:  36%|███▌      | 491/1378 [02:45<04:43,  3.13it/s]\n",
      "Loss: 35.54962:  36%|███▌      | 492/1378 [02:45<04:43,  3.13it/s]\n",
      "Loss: 35.44996:  36%|███▌      | 492/1378 [02:45<04:43,  3.13it/s]\n",
      "Loss: 35.44996:  36%|███▌      | 493/1378 [02:45<04:42,  3.13it/s]\n",
      "Loss: 34.15346:  36%|███▌      | 493/1378 [02:46<04:42,  3.13it/s]\n",
      "Loss: 34.15346:  36%|███▌      | 494/1378 [02:46<04:43,  3.12it/s]\n",
      "Loss: 32.83792:  36%|███▌      | 494/1378 [02:46<04:43,  3.12it/s]\n",
      "Loss: 32.83792:  36%|███▌      | 495/1378 [02:46<04:42,  3.12it/s]\n",
      "Loss: 34.84836:  36%|███▌      | 495/1378 [02:46<04:42,  3.12it/s]\n",
      "Loss: 34.84836:  36%|███▌      | 496/1378 [02:46<04:42,  3.13it/s]\n",
      "Loss: 35.14666:  36%|███▌      | 496/1378 [02:46<04:42,  3.13it/s]\n",
      "Loss: 35.14666:  36%|███▌      | 497/1378 [02:46<04:41,  3.13it/s]\n",
      "Loss: 33.66650:  36%|███▌      | 497/1378 [02:47<04:41,  3.13it/s]\n",
      "Loss: 33.66650:  36%|███▌      | 498/1378 [02:47<04:41,  3.13it/s]\n",
      "Loss: 35.94197:  36%|███▌      | 498/1378 [02:47<04:41,  3.13it/s]\n",
      "Loss: 35.94197:  36%|███▌      | 499/1378 [02:47<04:41,  3.12it/s]"
     ]
    }
   ],
   "source": [
    "!python3 -u ../src/main.py --mode train --img-dir ../Data/landmark-recognition-2020 --log-dir ../logs_final/ --model-dir ../weights_final/ --name mason_64px_64b_08ep_2000lb --min-img-per-label 2000 --img-dim 64 --epochs 8 --batch-size 64\n",
    "\n",
    "!python3 -u ../src/main.py --mode train --img-dir ../Data/landmark-recognition-2020 --log-dir ../logs_final/ --model-dir ../weights_final/ --name mason_64px_64b_08ep_1000lb --min-img-per-label 1000 --img-dim 64 --epochs 8 --batch-size 64\n",
    "\n",
    "!python3 -u ../src/main.py --mode train --img-dir ../Data/landmark-recognition-2020 --log-dir ../logs_final/ --model-dir ../weights_final/ --name mason_64px_64b_08ep_500lb --min-img-per-label 500 --img-dim 64 --epochs 8 --batch-size 64\n",
    "\n",
    "!python3 -u ../src/main.py --mode train --img-dir ../Data/landmark-recognition-2020 --log-dir ../logs_final/ --model-dir ../weights_final/ --name mason_64px_64b_08ep_250lb --min-img-per-label 250 --img-dim 64 --epochs 8 --batch-size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 45579 images.\n",
      "Out dimension is 51.\n",
      "Training set contains 31905 images.\n",
      "CV set contains 9116 images.\n",
      "Test set contains 4558 images.\n",
      "Fri Dec  4 18:20:28 2020 Epoch: 1\n",
      "\n",
      "  0%|          | 0/997 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"../src/main.py\", line 18, in <module>\n",
      "    lndmrk.train()\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\landmark.py\", line 111, in train\n",
      "    train_loss = self.train_epoch(model, train_loader, optimizer, loss_fn)\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\landmark.py\", line 154, in train_epoch\n",
      "    logits_m = model(data)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\models.py\", line 113, in forward\n",
      "    x = self.extract(x)\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\models.py\", line 110, in extract\n",
      "    return self.enet(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\gen_efficientnet.py\", line 266, in forward\n",
      "    x = self.features(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\gen_efficientnet.py\", line 251, in features\n",
      "    x = self.blocks(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\container.py\", line 117, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\container.py\", line 117, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\efficientnet_builder.py\", line 224, in forward\n",
      "    x = self.conv_pw(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\conv.py\", line 423, in forward\n",
      "    return self._conv_forward(input, self.weight)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\conv.py\", line 419, in _conv_forward\n",
      "    return F.conv2d(input, weight, self.bias, self.stride,\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 6.00 GiB total capacity; 4.18 GiB already allocated; 5.62 MiB free; 4.33 GiB reserved in total by PyTorch)\n",
      "\n",
      "Loaded 45579 images.\n",
      "Out dimension is 51.\n",
      "Training set contains 31905 images.\n",
      "CV set contains 9116 images.\n",
      "Test set contains 4558 images.\n",
      "Fri Dec  4 18:20:47 2020 Epoch: 1\n",
      "\n",
      "  0%|          | 0/997 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"../src/main.py\", line 18, in <module>\n",
      "    lndmrk.train()\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\landmark.py\", line 111, in train\n",
      "    train_loss = self.train_epoch(model, train_loader, optimizer, loss_fn)\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\landmark.py\", line 154, in train_epoch\n",
      "    logits_m = model(data)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\models.py\", line 113, in forward\n",
      "    x = self.extract(x)\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\models.py\", line 110, in extract\n",
      "    return self.enet(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\gen_efficientnet.py\", line 266, in forward\n",
      "    x = self.features(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\gen_efficientnet.py\", line 251, in features\n",
      "    x = self.blocks(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\container.py\", line 117, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\container.py\", line 117, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\efficientnet_builder.py\", line 226, in forward\n",
      "    x = self.act1(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\activations\\activations_jit.py\", line 48, in forward\n",
      "    return SwishJitAutoFn.apply(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\activations\\activations_jit.py\", line 29, in forward\n",
      "    return swish_jit_fwd(x)\n",
      "RuntimeError: The following operation failed in the TorchScript interpreter.\n",
      "Traceback of TorchScript (most recent call last):\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\activations\\activations_jit.py\", line 12, in fallback_function\n",
      "@torch.jit.script\n",
      "def swish_jit_fwd(x):\n",
      "    return x.mul(torch.sigmoid(x))\n",
      "                 ~~~~~~~~~~~~~ <--- HERE\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 6.00 GiB total capacity; 4.25 GiB already allocated; 9.62 MiB free; 4.33 GiB reserved in total by PyTorch)\n",
      "\n",
      "\n",
      "Loaded 15298 images.\n",
      "Out dimension is 7.\n",
      "Training set contains 10709 images.\n",
      "CV set contains 3060 images.\n",
      "Test set contains 1530 images.\n",
      "Fri Dec  4 18:21:07 2020 Epoch: 1\n",
      "\n",
      "  0%|          | 0/334 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"../src/main.py\", line 18, in <module>\n",
      "    lndmrk.train()\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\landmark.py\", line 111, in train\n",
      "    train_loss = self.train_epoch(model, train_loader, optimizer, loss_fn)\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\landmark.py\", line 154, in train_epoch\n",
      "    logits_m = model(data)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\models.py\", line 113, in forward\n",
      "    x = self.extract(x)\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\models.py\", line 110, in extract\n",
      "    return self.enet(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\gen_efficientnet.py\", line 266, in forward\n",
      "    x = self.features(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\gen_efficientnet.py\", line 251, in features\n",
      "    x = self.blocks(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\container.py\", line 117, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\container.py\", line 117, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\efficientnet_builder.py\", line 226, in forward\n",
      "    x = self.act1(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\activations\\activations_jit.py\", line 48, in forward\n",
      "    return SwishJitAutoFn.apply(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\activations\\activations_jit.py\", line 29, in forward\n",
      "    return swish_jit_fwd(x)\n",
      "RuntimeError: The following operation failed in the TorchScript interpreter.\n",
      "Traceback of TorchScript (most recent call last):\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\activations\\activations_jit.py\", line 12, in fallback_function\n",
      "@torch.jit.script\n",
      "def swish_jit_fwd(x):\n",
      "    return x.mul(torch.sigmoid(x))\n",
      "                 ~~~~~~~~~~~~~ <--- HERE\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 6.00 GiB total capacity; 4.25 GiB already allocated; 9.62 MiB free; 4.33 GiB reserved in total by PyTorch)\n",
      "\n",
      "\n",
      "Loaded 8503 images.\n",
      "Out dimension is 2.\n",
      "Training set contains 5952 images.\n",
      "CV set contains 1701 images.\n",
      "Test set contains 850 images.\n",
      "Fri Dec  4 18:21:25 2020 Epoch: 1\n",
      "\n",
      "  0%|          | 0/186 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"../src/main.py\", line 18, in <module>\n",
      "    lndmrk.train()\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\landmark.py\", line 111, in train\n",
      "    train_loss = self.train_epoch(model, train_loader, optimizer, loss_fn)\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\landmark.py\", line 154, in train_epoch\n",
      "    logits_m = model(data)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\models.py\", line 113, in forward\n",
      "    x = self.extract(x)\n",
      "  File \"c:\\Users\\Mason\\Downloads\\Kaggle\\landmark\\src\\models.py\", line 110, in extract\n",
      "    return self.enet(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\gen_efficientnet.py\", line 266, in forward\n",
      "    x = self.features(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\gen_efficientnet.py\", line 251, in features\n",
      "    x = self.blocks(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\container.py\", line 117, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\container.py\", line 117, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\geffnet\\efficientnet_builder.py\", line 173, in forward\n",
      "    x = self.bn2(x)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\batchnorm.py\", line 131, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"C:\\Users\\Mason\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\functional.py\", line 2056, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 6.00 GiB total capacity; 4.19 GiB already allocated; 129.62 MiB free; 4.21 GiB reserved in total by PyTorch)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -u ../src/main.py --mode train --img-dir ../Data/landmark-recognition-2020 --log-dir ../logs_final/ --model-dir ../weights_final/ --name mason_128px_32b_08ep_500lb --min-img-per-label 500 --img-dim 128 --epochs 8 --batch-size 32\n",
    "\n",
    "!python3 -u ../src/main.py --mode train --img-dir ../Data/landmark-recognition-2020 --log-dir ../logs_final/ --model-dir ../weights_final/ --name mason_256px_32b_08ep_500lb --min-img-per-label 500 --img-dim 256 --epochs 8 --batch-size 32\n",
    "\n",
    "!python3 -u ../src/main.py --mode train --img-dir ../Data/landmark-recognition-2020 --log-dir ../logs_final/ --model-dir ../weights_final/ --name mason_256px_32b_06ep_1000lb --min-img-per-label 1000 --img-dim 256 --epochs 6 --batch-size 32\n",
    "\n",
    "!python3 -u ../src/main.py --mode train --img-dir ../Data/landmark-recognition-2020 --log-dir ../logs_final/ --model-dir ../weights_final/ --name mason_512px_32b_04ep_2000lb --min-img-per-label 2000 --img-dim 512 --epochs 4 --batch-size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}