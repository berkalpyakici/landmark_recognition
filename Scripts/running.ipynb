{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"C:\\Users\\Mason\\Downloads\\Kaggle\\Google Landmark Classification 2020\\Data\\landmark-recognition-2020\"\n",
    "model_dir=./weights\n",
    "\n",
    "# \"C:\\Users\\Mason\\Downloads\\Kaggle\\Google Landmark Classification 2020\\Scripts\\train.py\"\n",
    "\n",
    "### B7 \n",
    "\n",
    "python -u Scripts/train.py --kernel-type b7ns_DDP_final_32_300w_f0_2ep --train-step 0 --data-dir Data/landmark-recognition-2020 --image-size 32 --batch-size 42 --enet-type tf_efficientnet_b7_ns --n-epochs 2 --CUDA_VISIBLE_DEVICES 0 --fold 0 --local_rank 0\n",
    "\n",
    "# python -u -m torch.distributed.launch --nproc_per_node=8 train.py --kernel-type b7ns_DDP_final_512_300w_f0_40ep --train-step 1 --data-dir ${data_dir} --image-size 512 --batch-size 16 --enet-type tf_efficientnet_b7_ns --n-epochs 40 --stop-at-epoch 13 --fold 0 --load-from ${model_dir}/b7ns_DDP_final_256_300w_f0_10ep_fold0.pth\n",
    "\n",
    "# python -u -m torch.distributed.launch --nproc_per_node=8 train.py --kernel-type b7ns_final_672_300w_f0_load13_ep20 --train-step 2 --data-dir ${data_dir} --init-lr 5e-5 --image-size 672 --batch-size 10 --enet-type tf_efficientnet_b7_ns --n-epochs 20 --stop-at-epoch 1 --fold 0 --load-from ${model_dir}/b7ns_DDP_final_512_300w_f0_40ep_fold0.pth\n",
    "\n",
    "# python -u -m torch.distributed.launch --nproc_per_node=8 train.py --kernel-type b7ns_final_672_300w_f0_load13_load1_14ep --train-step 3 --data-dir ${data_dir} --image-size 672 --batch-size 10 --enet-type tf_efficientnet_b7_ns --n-epochs 14 --stop-at-epoch 4 --fold 0 --load-from ${model_dir}/b7ns_final_672_300w_f0_load13_ep20_fold0.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                        id                                                url  \\\n",
       "0        6e158a47eb2ca3f6  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "1        202cd79556f30760  http://upload.wikimedia.org/wikipedia/commons/...   \n",
       "2        3ad87684c99c06e1  http://upload.wikimedia.org/wikipedia/commons/...   \n",
       "3        e7f70e9c61e66af3  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "4        4072182eddd0100e  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "...                   ...                                                ...   \n",
       "4132909  fc0f007893b11ba7  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "4132910  39aad18585867916  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "4132911  fd0725460e4ebbec  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "4132912  73691ae29e24ba19  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "4132913  8ef8dff6fc4790c2  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "\n",
       "         landmark_id  \n",
       "0             142820  \n",
       "1             104169  \n",
       "2              37914  \n",
       "3             102140  \n",
       "4               2474  \n",
       "...              ...  \n",
       "4132909       172138  \n",
       "4132910       162860  \n",
       "4132911       191243  \n",
       "4132912       145760  \n",
       "4132913        34698  \n",
       "\n",
       "[4132914 rows x 3 columns]>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Mason/Downloads/Kaggle/Google Landmark Classification 2020/Data/landmark-recognition-2020/train.csv\")\n",
    "\n",
    "subset = df.sample(frac=0.001,random_state=200)\n",
    "print(subset.shape)\n",
    "\n",
    "subset.to_csv('C:/Users/Mason/Downloads/Kaggle/Google Landmark Classification 2020/Data/landmark-recognition-2020/train_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'GeForce RTX 2060'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import torch\n",
    "## Get Id of default device\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting environment information...\n",
      "PyTorch version: 1.7.0\n",
      "Is debug build: True\n",
      "CUDA used to build PyTorch: 10.2\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 10 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: Could not collect\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "GPU models and configuration: Could not collect\n",
      "Nvidia driver version: Could not collect\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.18.5\n",
      "[pip3] numpydoc==1.1.0\n",
      "[pip3] torch==1.7.0\n",
      "[pip3] torchaudio==0.7.0\n",
      "[pip3] torchvision==0.8.1\n",
      "[conda] blas                      1.0                         mkl  \n",
      "[conda] cudatoolkit               10.2.89              h74a9793_1  \n",
      "[conda] mkl                       2020.1                      216  \n",
      "[conda] mkl-service               2.3.0            py38hb782905_0  \n",
      "[conda] mkl_fft                   1.1.0            py38h45dec08_0  \n",
      "[conda] mkl_random                1.1.1            py38h47e9c7a_0  \n",
      "[conda] numpy                     1.18.5           py38h6530119_0  \n",
      "[conda] numpy-base                1.18.5           py38hc3f5095_0  \n",
      "[conda] numpydoc                  1.1.0                      py_0  \n",
      "[conda] pytorch                   1.7.0           py3.8_cuda102_cudnn7_0    pytorch\n",
      "[conda] torchaudio                0.7.0                      py38    pytorch\n",
      "[conda] torchvision               0.8.1                py38_cu102    pytorch\n"
     ]
    }
   ],
   "source": [
    "# This script outputs relevant system environment info\n",
    "# Run it with `python collect_env.py`.\n",
    "import locale\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except (ImportError, NameError, AttributeError, OSError):\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "# System Environment Information\n",
    "SystemEnv = namedtuple('SystemEnv', [\n",
    "    'torch_version',\n",
    "    'is_debug_build',\n",
    "    'cuda_compiled_version',\n",
    "    'gcc_version',\n",
    "    'clang_version',\n",
    "    'cmake_version',\n",
    "    'os',\n",
    "    'python_version',\n",
    "    'is_cuda_available',\n",
    "    'cuda_runtime_version',\n",
    "    'nvidia_driver_version',\n",
    "    'nvidia_gpu_models',\n",
    "    'cudnn_version',\n",
    "    'pip_version',  # 'pip' or 'pip3'\n",
    "    'pip_packages',\n",
    "    'conda_packages',\n",
    "    'hip_compiled_version',\n",
    "    'hip_runtime_version',\n",
    "    'miopen_runtime_version',\n",
    "])\n",
    "\n",
    "\n",
    "def run(command):\n",
    "    \"\"\"Returns (return-code, stdout, stderr)\"\"\"\n",
    "    p = subprocess.Popen(command, stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.PIPE, shell=True)\n",
    "    raw_output, raw_err = p.communicate()\n",
    "    rc = p.returncode\n",
    "    enc = locale.getpreferredencoding()\n",
    "    output = raw_output.decode(enc)\n",
    "    err = raw_err.decode(enc)\n",
    "    return rc, output.strip(), err.strip()\n",
    "\n",
    "\n",
    "def run_and_read_all(run_lambda, command):\n",
    "    \"\"\"Runs command using run_lambda; reads and returns entire output if rc is 0\"\"\"\n",
    "    rc, out, _ = run_lambda(command)\n",
    "    if rc != 0:\n",
    "        return None\n",
    "    return out\n",
    "\n",
    "\n",
    "def run_and_parse_first_match(run_lambda, command, regex):\n",
    "    \"\"\"Runs command using run_lambda, returns the first regex match if it exists\"\"\"\n",
    "    rc, out, _ = run_lambda(command)\n",
    "    if rc != 0:\n",
    "        return None\n",
    "    match = re.search(regex, out)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return match.group(1)\n",
    "\n",
    "\n",
    "def get_conda_packages(run_lambda):\n",
    "    if get_platform() == 'win32':\n",
    "        system_root = os.environ.get('SystemRoot', 'C:\\\\Windows')\n",
    "        findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n",
    "        grep_cmd = r'{} /R \"torch numpy cudatoolkit soumith mkl magma\"'.format(findstr_cmd)\n",
    "    else:\n",
    "        grep_cmd = r'grep \"torch\\|numpy\\|cudatoolkit\\|soumith\\|mkl\\|magma\"'\n",
    "    conda = os.environ.get('CONDA_EXE', 'conda')\n",
    "    out = run_and_read_all(run_lambda, conda + ' list | ' + grep_cmd)\n",
    "    if out is None:\n",
    "        return out\n",
    "    # Comment starting at beginning of line\n",
    "    comment_regex = re.compile(r'^#.*\\n')\n",
    "    return re.sub(comment_regex, '', out)\n",
    "\n",
    "\n",
    "def get_gcc_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'gcc --version', r'gcc (.*)')\n",
    "\n",
    "def get_clang_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'clang --version', r'clang version (.*)')\n",
    "\n",
    "\n",
    "def get_cmake_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'cmake --version', r'cmake (.*)')\n",
    "\n",
    "\n",
    "def get_nvidia_driver_version(run_lambda):\n",
    "    if get_platform() == 'darwin':\n",
    "        cmd = 'kextstat | grep -i cuda'\n",
    "        return run_and_parse_first_match(run_lambda, cmd,\n",
    "                                         r'com[.]nvidia[.]CUDA [(](.*?)[)]')\n",
    "    smi = get_nvidia_smi()\n",
    "    return run_and_parse_first_match(run_lambda, smi, r'Driver Version: (.*?) ')\n",
    "\n",
    "\n",
    "def get_gpu_info(run_lambda):\n",
    "    if get_platform() == 'darwin' or (TORCH_AVAILABLE and torch.version.hip is not None):\n",
    "        if TORCH_AVAILABLE and torch.cuda.is_available():\n",
    "            return torch.cuda.get_device_name(None)\n",
    "        return None\n",
    "    smi = get_nvidia_smi()\n",
    "    uuid_regex = re.compile(r' \\(UUID: .+?\\)')\n",
    "    rc, out, _ = run_lambda(smi + ' -L')\n",
    "    if rc != 0:\n",
    "        return None\n",
    "    # Anonymize GPUs by removing their UUID\n",
    "    return re.sub(uuid_regex, '', out)\n",
    "\n",
    "\n",
    "def get_running_cuda_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'nvcc --version', r'V(.*)$')\n",
    "\n",
    "\n",
    "def get_cudnn_version(run_lambda):\n",
    "    \"\"\"This will return a list of libcudnn.so; it's hard to tell which one is being used\"\"\"\n",
    "    if get_platform() == 'win32':\n",
    "        system_root = os.environ.get('SystemRoot', 'C:\\\\Windows')\n",
    "        cuda_path = os.environ.get('CUDA_PATH', \"%CUDA_PATH%\")\n",
    "        where_cmd = os.path.join(system_root, 'System32', 'where')\n",
    "        cudnn_cmd = '{} /R \"{}\\\\bin\" cudnn*.dll'.format(where_cmd, cuda_path)\n",
    "    elif get_platform() == 'darwin':\n",
    "        # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n",
    "        # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n",
    "        # https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installmac\n",
    "        # Use CUDNN_LIBRARY when cudnn library is installed elsewhere.\n",
    "        cudnn_cmd = 'ls /usr/local/cuda/lib/libcudnn*'\n",
    "    else:\n",
    "        cudnn_cmd = 'ldconfig -p | grep libcudnn | rev | cut -d\" \" -f1 | rev'\n",
    "    rc, out, _ = run_lambda(cudnn_cmd)\n",
    "    # find will return 1 if there are permission errors or if not found\n",
    "    if len(out) == 0 or (rc != 1 and rc != 0):\n",
    "        l = os.environ.get('CUDNN_LIBRARY')\n",
    "        if l is not None and os.path.isfile(l):\n",
    "            return os.path.realpath(l)\n",
    "        return None\n",
    "    files_set = set()\n",
    "    for fn in out.split('\\n'):\n",
    "        fn = os.path.realpath(fn)  # eliminate symbolic links\n",
    "        if os.path.isfile(fn):\n",
    "            files_set.add(fn)\n",
    "    if not files_set:\n",
    "        return None\n",
    "    # Alphabetize the result because the order is non-deterministic otherwise\n",
    "    files = list(sorted(files_set))\n",
    "    if len(files) == 1:\n",
    "        return files[0]\n",
    "    result = '\\n'.join(files)\n",
    "    return 'Probably one of the following:\\n{}'.format(result)\n",
    "\n",
    "\n",
    "def get_nvidia_smi():\n",
    "    # Note: nvidia-smi is currently available only on Windows and Linux\n",
    "    smi = 'nvidia-smi'\n",
    "    if get_platform() == 'win32':\n",
    "        smi = '\"C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVSMI\\\\%s\"' % smi\n",
    "    return smi\n",
    "\n",
    "\n",
    "def get_platform():\n",
    "    if sys.platform.startswith('linux'):\n",
    "        return 'linux'\n",
    "    elif sys.platform.startswith('win32'):\n",
    "        return 'win32'\n",
    "    elif sys.platform.startswith('cygwin'):\n",
    "        return 'cygwin'\n",
    "    elif sys.platform.startswith('darwin'):\n",
    "        return 'darwin'\n",
    "    else:\n",
    "        return sys.platform\n",
    "\n",
    "\n",
    "def get_mac_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'sw_vers -productVersion', r'(.*)')\n",
    "\n",
    "\n",
    "def get_windows_version(run_lambda):\n",
    "    system_root = os.environ.get('SystemRoot', 'C:\\\\Windows')\n",
    "    wmic_cmd = os.path.join(system_root, 'System32', 'Wbem', 'wmic')\n",
    "    findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n",
    "    return run_and_read_all(run_lambda, '{} os get Caption | {} /v Caption'.format(wmic_cmd, findstr_cmd))\n",
    "\n",
    "\n",
    "def get_lsb_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'lsb_release -a', r'Description:\\t(.*)')\n",
    "\n",
    "\n",
    "def check_release_file(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'cat /etc/*-release',\n",
    "                                     r'PRETTY_NAME=\"(.*)\"')\n",
    "\n",
    "\n",
    "def get_os(run_lambda):\n",
    "    from platform import machine\n",
    "    platform = get_platform()\n",
    "\n",
    "    if platform == 'win32' or platform == 'cygwin':\n",
    "        return get_windows_version(run_lambda)\n",
    "\n",
    "    if platform == 'darwin':\n",
    "        version = get_mac_version(run_lambda)\n",
    "        if version is None:\n",
    "            return None\n",
    "        return 'Mac OSX {} ({})'.format(version, machine())\n",
    "\n",
    "    if platform == 'linux':\n",
    "        # Ubuntu/Debian based\n",
    "        desc = get_lsb_version(run_lambda)\n",
    "        if desc is not None:\n",
    "            return '{} ({})'.format(desc, machine())\n",
    "\n",
    "        # Try reading /etc/*-release\n",
    "        desc = check_release_file(run_lambda)\n",
    "        if desc is not None:\n",
    "            return '{} ({})'.format(desc, machine())\n",
    "\n",
    "        return '{} ({})'.format(platform, machine())\n",
    "\n",
    "    # Unknown platform\n",
    "    return platform\n",
    "\n",
    "\n",
    "def get_pip_packages(run_lambda):\n",
    "    \"\"\"Returns `pip list` output. Note: will also find conda-installed pytorch\n",
    "    and numpy packages.\"\"\"\n",
    "    # People generally have `pip` as `pip` or `pip3`\n",
    "    def run_with_pip(pip):\n",
    "        if get_platform() == 'win32':\n",
    "            system_root = os.environ.get('SystemRoot', 'C:\\\\Windows')\n",
    "            findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n",
    "            grep_cmd = r'{} /R \"numpy torch\"'.format(findstr_cmd)\n",
    "        else:\n",
    "            grep_cmd = r'grep \"torch\\|numpy\"'\n",
    "        return run_and_read_all(run_lambda, pip + ' list --format=freeze | ' + grep_cmd)\n",
    "\n",
    "    # Try to figure out if the user is running pip or pip3.\n",
    "    out2 = run_with_pip('pip')\n",
    "    out3 = run_with_pip('pip3')\n",
    "\n",
    "    num_pips = len([x for x in [out2, out3] if x is not None])\n",
    "    if num_pips == 0:\n",
    "        return 'pip', out2\n",
    "\n",
    "    if num_pips == 1:\n",
    "        if out2 is not None:\n",
    "            return 'pip', out2\n",
    "        return 'pip3', out3\n",
    "\n",
    "    # num_pips is 2. Return pip3 by default b/c that most likely\n",
    "    # is the one associated with Python 3\n",
    "    return 'pip3', out3\n",
    "\n",
    "\n",
    "def get_env_info():\n",
    "    run_lambda = run\n",
    "    pip_version, pip_list_output = get_pip_packages(run_lambda)\n",
    "\n",
    "    if TORCH_AVAILABLE:\n",
    "        version_str = torch.__version__\n",
    "        debug_mode_str = str(torch.version.debug)\n",
    "        cuda_available_str = str(torch.cuda.is_available())\n",
    "        cuda_version_str = torch.version.cuda\n",
    "        if torch.version.hip is None:  # cuda version\n",
    "            hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'\n",
    "        else:  # HIP version\n",
    "            cfg = torch._C._show_config().split('\\n')\n",
    "            hip_runtime_version = [s.rsplit(None, 1)[-1] for s in cfg if 'HIP Runtime' in s][0]\n",
    "            miopen_runtime_version = [s.rsplit(None, 1)[-1] for s in cfg if 'MIOpen' in s][0]\n",
    "            cuda_version_str = 'N/A'\n",
    "            hip_compiled_version = torch.version.hip\n",
    "    else:\n",
    "        version_str = debug_mode_str = cuda_available_str = cuda_version_str = 'N/A'\n",
    "        hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'\n",
    "\n",
    "    return SystemEnv(\n",
    "        torch_version=version_str,\n",
    "        is_debug_build=debug_mode_str,\n",
    "        python_version='{}.{} ({}-bit runtime)'.format(sys.version_info[0], sys.version_info[1], sys.maxsize.bit_length() + 1),\n",
    "        is_cuda_available=cuda_available_str,\n",
    "        cuda_compiled_version=cuda_version_str,\n",
    "        cuda_runtime_version=get_running_cuda_version(run_lambda),\n",
    "        nvidia_gpu_models=get_gpu_info(run_lambda),\n",
    "        nvidia_driver_version=get_nvidia_driver_version(run_lambda),\n",
    "        cudnn_version=get_cudnn_version(run_lambda),\n",
    "        hip_compiled_version=hip_compiled_version,\n",
    "        hip_runtime_version=hip_runtime_version,\n",
    "        miopen_runtime_version=miopen_runtime_version,\n",
    "        pip_version=pip_version,\n",
    "        pip_packages=pip_list_output,\n",
    "        conda_packages=get_conda_packages(run_lambda),\n",
    "        os=get_os(run_lambda),\n",
    "        gcc_version=get_gcc_version(run_lambda),\n",
    "        clang_version=get_clang_version(run_lambda),\n",
    "        cmake_version=get_cmake_version(run_lambda),\n",
    "    )\n",
    "\n",
    "env_info_fmt = \"\"\"\n",
    "PyTorch version: {torch_version}\n",
    "Is debug build: {is_debug_build}\n",
    "CUDA used to build PyTorch: {cuda_compiled_version}\n",
    "ROCM used to build PyTorch: {hip_compiled_version}\n",
    "\n",
    "OS: {os}\n",
    "GCC version: {gcc_version}\n",
    "Clang version: {clang_version}\n",
    "CMake version: {cmake_version}\n",
    "\n",
    "Python version: {python_version}\n",
    "Is CUDA available: {is_cuda_available}\n",
    "CUDA runtime version: {cuda_runtime_version}\n",
    "GPU models and configuration: {nvidia_gpu_models}\n",
    "Nvidia driver version: {nvidia_driver_version}\n",
    "cuDNN version: {cudnn_version}\n",
    "HIP runtime version: {hip_runtime_version}\n",
    "MIOpen runtime version: {miopen_runtime_version}\n",
    "\n",
    "Versions of relevant libraries:\n",
    "{pip_packages}\n",
    "{conda_packages}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def pretty_str(envinfo):\n",
    "    def replace_nones(dct, replacement='Could not collect'):\n",
    "        for key in dct.keys():\n",
    "            if dct[key] is not None:\n",
    "                continue\n",
    "            dct[key] = replacement\n",
    "        return dct\n",
    "\n",
    "    def replace_bools(dct, true='Yes', false='No'):\n",
    "        for key in dct.keys():\n",
    "            if dct[key] is True:\n",
    "                dct[key] = true\n",
    "            elif dct[key] is False:\n",
    "                dct[key] = false\n",
    "        return dct\n",
    "\n",
    "    def prepend(text, tag='[prepend]'):\n",
    "        lines = text.split('\\n')\n",
    "        updated_lines = [tag + line for line in lines]\n",
    "        return '\\n'.join(updated_lines)\n",
    "\n",
    "    def replace_if_empty(text, replacement='No relevant packages'):\n",
    "        if text is not None and len(text) == 0:\n",
    "            return replacement\n",
    "        return text\n",
    "\n",
    "    def maybe_start_on_next_line(string):\n",
    "        # If `string` is multiline, prepend a \\n to it.\n",
    "        if string is not None and len(string.split('\\n')) > 1:\n",
    "            return '\\n{}\\n'.format(string)\n",
    "        return string\n",
    "\n",
    "    mutable_dict = envinfo._asdict()\n",
    "\n",
    "    # If nvidia_gpu_models is multiline, start on the next line\n",
    "    mutable_dict['nvidia_gpu_models'] = \\\n",
    "        maybe_start_on_next_line(envinfo.nvidia_gpu_models)\n",
    "\n",
    "    # If the machine doesn't have CUDA, report some fields as 'No CUDA'\n",
    "    dynamic_cuda_fields = [\n",
    "        'cuda_runtime_version',\n",
    "        'nvidia_gpu_models',\n",
    "        'nvidia_driver_version',\n",
    "    ]\n",
    "    all_cuda_fields = dynamic_cuda_fields + ['cudnn_version']\n",
    "    all_dynamic_cuda_fields_missing = all(\n",
    "        mutable_dict[field] is None for field in dynamic_cuda_fields)\n",
    "    if TORCH_AVAILABLE and not torch.cuda.is_available() and all_dynamic_cuda_fields_missing:\n",
    "        for field in all_cuda_fields:\n",
    "            mutable_dict[field] = 'No CUDA'\n",
    "        if envinfo.cuda_compiled_version is None:\n",
    "            mutable_dict['cuda_compiled_version'] = 'None'\n",
    "\n",
    "    # Replace True with Yes, False with No\n",
    "    mutable_dict = replace_bools(mutable_dict)\n",
    "\n",
    "    # Replace all None objects with 'Could not collect'\n",
    "    mutable_dict = replace_nones(mutable_dict)\n",
    "\n",
    "    # If either of these are '', replace with 'No relevant packages'\n",
    "    mutable_dict['pip_packages'] = replace_if_empty(mutable_dict['pip_packages'])\n",
    "    mutable_dict['conda_packages'] = replace_if_empty(mutable_dict['conda_packages'])\n",
    "\n",
    "    # Tag conda and pip packages with a prefix\n",
    "    # If they were previously None, they'll show up as ie '[conda] Could not collect'\n",
    "    if mutable_dict['pip_packages']:\n",
    "        mutable_dict['pip_packages'] = prepend(mutable_dict['pip_packages'],\n",
    "                                               '[{}] '.format(envinfo.pip_version))\n",
    "    if mutable_dict['conda_packages']:\n",
    "        mutable_dict['conda_packages'] = prepend(mutable_dict['conda_packages'],\n",
    "                                                 '[conda] ')\n",
    "    return env_info_fmt.format(**mutable_dict)\n",
    "\n",
    "\n",
    "def get_pretty_env_info():\n",
    "    return pretty_str(get_env_info())\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Collecting environment information...\")\n",
    "    output = get_pretty_env_info()\n",
    "    print(output)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}